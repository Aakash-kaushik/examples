{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://lab.mlpack.org/v2/gh/mlpack/examples/master?urlpath=lab%2Ftree%2Fq_learning%2Facrobot_dqn.ipynb)\n",
    "\n",
    "You can easily run this notebook at https://lab.mlpack.org/\n",
    "\n",
    "This notebook is shows how to get use 3-Step Double DQN with Prioritized Replay to train an agent to get high scores for the [Acrobot](https://gym.openai.com/envs/Acrobot-v1) environment. \n",
    "\n",
    "We make the agent train and test on OpenAI Gym toolkit's GUI interface provided through a distributed infrastructure (TCP API). More details can be found [here](https://github.com/zoq/gym_tcp_api).\n",
    "\n",
    "A video of the trained agent can be seen in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including necessary libraries and namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/core.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/methods/ann/ffn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_learning.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_networks/simple_dqn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/environment/env_type.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/training_config.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to run the agent on gym's environment (provided externally) for testing.\n",
    "#include <gym/environment.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to generate and display a video of the trained agent.\n",
    "#include \"xwidgets/ximage.hpp\"\n",
    "#include \"xwidgets/xvideo.hpp\"\n",
    "#include \"xwidgets/xaudio.hpp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::ann;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace ens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::rl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the state and action space.\n",
    "DiscreteActionEnv::State::dimension = 6;\n",
    "DiscreteActionEnv::Action::size = 3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the network.\n",
    "SimpleDQN<> model(DiscreteActionEnv::State::dimension, 64, 32,\n",
    "                  DiscreteActionEnv::Action::size);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the policy method.\n",
    "GreedyPolicy<DiscreteActionEnv> policy(1.0, 1000, 0.1, 0.99);\n",
    "// To enable 3-step learning, we set the last parameter of the replay method as 3.\n",
    "PrioritizedReplay<DiscreteActionEnv> replayMethod(64, 5000, 0.6, 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up training configurations.\n",
    "TrainingConfig config;\n",
    "config.TargetNetworkSyncInterval() = 100;\n",
    "config.ExplorationSteps() = 500;\n",
    "\n",
    "// We use double Q learning for this example.\n",
    "config.DoubleQLearning() = true;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up DQN agent.\n",
    "QLearning<DiscreteActionEnv, decltype(model), AdamUpdate, decltype(policy), decltype(replayMethod)>\n",
    "    agent(config, model, policy, replayMethod);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the gym training environment.\n",
    "gym::Environment env(\"gym.kurg.org\", \"4040\", \"Acrobot-v1\");\n",
    "\n",
    "// Initializing training variables.\n",
    "std::vector<double> returnList;\n",
    "size_t episodes = 0;\n",
    "bool converged = true;\n",
    "\n",
    "// The number of episode returns to keep track of.\n",
    "size_t consecutiveEpisodes = 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Function to train the agent on mlpack's own implementation of the CartPole environment.\n",
    "void train(const size_t numSteps)\n",
    "{\n",
    "  agent.Deterministic() = false;\n",
    "  std::cout << \"Training for \" << numSteps << \" steps.\" << std::endl;\n",
    "  while (agent.TotalSteps() < numSteps)\n",
    "  {\n",
    "    double episodeReturn = 0;\n",
    "    env.reset();\n",
    "    do\n",
    "    {\n",
    "      agent.State().Data() = env.observation;\n",
    "      agent.SelectAction();\n",
    "      arma::mat action = {double(agent.Action().action)};\n",
    "\n",
    "      env.step(action);\n",
    "      DiscreteActionEnv::State nextState;\n",
    "      nextState.Data() = env.observation;\n",
    "\n",
    "      replayMethod.Store(agent.State(), agent.Action(), env.reward, nextState,\n",
    "          env.done, 0.99);\n",
    "      episodeReturn += env.reward;\n",
    "      agent.TotalSteps()++;\n",
    "      if (agent.Deterministic() || agent.TotalSteps() < config.ExplorationSteps())\n",
    "        continue;\n",
    "      agent.TrainAgent();\n",
    "    } while (!env.done);\n",
    "    returnList.push_back(episodeReturn);\n",
    "    episodes += 1;\n",
    "\n",
    "    if (returnList.size() > consecutiveEpisodes)\n",
    "      returnList.erase(returnList.begin());\n",
    "        \n",
    "    double averageReturn = std::accumulate(returnList.begin(),\n",
    "                                           returnList.end(), 0.0) /\n",
    "                           returnList.size();\n",
    "    if(episodes % 1 == 0)\n",
    "    {\n",
    "      std::cout << \"Avg return in last \" << consecutiveEpisodes\n",
    "          << \" episodes: \" << averageReturn\n",
    "          << \"\\t Episode return: \" << episodeReturn\n",
    "          << \"\\t Total steps: \" << agent.TotalSteps() << std::endl;\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let the training begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps.\n",
      "Avg return in last 50 episodes: -500\t Episode return: -500\t Total steps: 500\n",
      "Avg return in last 50 episodes: -500\t Episode return: -500\t Total steps: 1000\n",
      "Avg return in last 50 episodes: -406\t Episode return: -218\t Total steps: 1219\n",
      "Avg return in last 50 episodes: -396.75\t Episode return: -369\t Total steps: 1589\n",
      "Avg return in last 50 episodes: -370\t Episode return: -263\t Total steps: 1853\n",
      "Avg return in last 50 episodes: -369.333\t Episode return: -366\t Total steps: 2220\n",
      "Avg return in last 50 episodes: -340.429\t Episode return: -167\t Total steps: 2388\n",
      "Avg return in last 50 episodes: -337.375\t Episode return: -316\t Total steps: 2705\n",
      "Avg return in last 50 episodes: -334.444\t Episode return: -311\t Total steps: 3017\n",
      "Avg return in last 50 episodes: -316.1\t Episode return: -151\t Total steps: 3169\n",
      "Avg return in last 50 episodes: -307.273\t Episode return: -219\t Total steps: 3389\n",
      "Avg return in last 50 episodes: -310.5\t Episode return: -346\t Total steps: 3736\n",
      "Avg return in last 50 episodes: -303.769\t Episode return: -223\t Total steps: 3960\n",
      "Avg return in last 50 episodes: -297.786\t Episode return: -220\t Total steps: 4181\n",
      "Avg return in last 50 episodes: -291.933\t Episode return: -210\t Total steps: 4392\n",
      "Avg return in last 50 episodes: -286.5\t Episode return: -205\t Total steps: 4598\n",
      "Avg return in last 50 episodes: -299.059\t Episode return: -500\t Total steps: 5098\n"
     ]
    }
   ],
   "source": [
    "// Training the agent for a total of at least 5000 steps.\n",
    "train(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total steps: 500\t Total reward: -500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731583bf72eb4b23ba3402d73b8c7338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Deterministic() = true;\n",
    "\n",
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment envTest(\"gym.kurg.org\", \"4040\", \"Acrobot-v1\");\n",
    "envTest.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {double(agent.Action().action)};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += envTest.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little more training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 15000 steps.\n",
      "Avg return in last 50 episodes: -294\t Episode return: -208\t Total steps: 5307\n",
      "Avg return in last 50 episodes: -284.684\t Episode return: -117\t Total steps: 5425\n",
      "Avg return in last 50 episodes: -277.25\t Episode return: -136\t Total steps: 5562\n",
      "Avg return in last 50 episodes: -269.143\t Episode return: -107\t Total steps: 5670\n",
      "Avg return in last 50 episodes: -264.409\t Episode return: -165\t Total steps: 5836\n",
      "Avg return in last 50 episodes: -257.696\t Episode return: -110\t Total steps: 5947\n",
      "Avg return in last 50 episodes: -253.333\t Episode return: -153\t Total steps: 6101\n",
      "Avg return in last 50 episodes: -256.36\t Episode return: -329\t Total steps: 6431\n",
      "Avg return in last 50 episodes: -250\t Episode return: -91\t Total steps: 6523\n",
      "Avg return in last 50 episodes: -244.556\t Episode return: -103\t Total steps: 6627\n",
      "Avg return in last 50 episodes: -241.536\t Episode return: -160\t Total steps: 6788\n",
      "Avg return in last 50 episodes: -238.483\t Episode return: -153\t Total steps: 6942\n",
      "Avg return in last 50 episodes: -234.433\t Episode return: -117\t Total steps: 7060\n",
      "Avg return in last 50 episodes: -230.806\t Episode return: -122\t Total steps: 7183\n",
      "Avg return in last 50 episodes: -227.969\t Episode return: -140\t Total steps: 7324\n",
      "Avg return in last 50 episodes: -226.758\t Episode return: -188\t Total steps: 7513\n",
      "Avg return in last 50 episodes: -224.559\t Episode return: -152\t Total steps: 7666\n",
      "Avg return in last 50 episodes: -224.029\t Episode return: -206\t Total steps: 7873\n",
      "Avg return in last 50 episodes: -224.917\t Episode return: -256\t Total steps: 8130\n",
      "Avg return in last 50 episodes: -223.135\t Episode return: -159\t Total steps: 8290\n",
      "Avg return in last 50 episodes: -221.132\t Episode return: -147\t Total steps: 8438\n",
      "Avg return in last 50 episodes: -218.513\t Episode return: -119\t Total steps: 8558\n",
      "Avg return in last 50 episodes: -216.725\t Episode return: -147\t Total steps: 8706\n",
      "Avg return in last 50 episodes: -213.537\t Episode return: -86\t Total steps: 8793\n",
      "Avg return in last 50 episodes: -212.786\t Episode return: -182\t Total steps: 8976\n",
      "Avg return in last 50 episodes: -210.814\t Episode return: -128\t Total steps: 9105\n",
      "Avg return in last 50 episodes: -208.955\t Episode return: -129\t Total steps: 9235\n",
      "Avg return in last 50 episodes: -206.444\t Episode return: -96\t Total steps: 9332\n",
      "Avg return in last 50 episodes: -205\t Episode return: -140\t Total steps: 9473\n",
      "Avg return in last 50 episodes: -203.447\t Episode return: -132\t Total steps: 9606\n",
      "Avg return in last 50 episodes: -203.083\t Episode return: -186\t Total steps: 9793\n",
      "Avg return in last 50 episodes: -201.837\t Episode return: -142\t Total steps: 9936\n",
      "Avg return in last 50 episodes: -200.86\t Episode return: -153\t Total steps: 10090\n",
      "Avg return in last 50 episodes: -194.68\t Episode return: -191\t Total steps: 10282\n",
      "Avg return in last 50 episodes: -187.64\t Episode return: -148\t Total steps: 10431\n",
      "Avg return in last 50 episodes: -186.04\t Episode return: -138\t Total steps: 10570\n",
      "Avg return in last 50 episodes: -181.3\t Episode return: -132\t Total steps: 10703\n",
      "Avg return in last 50 episodes: -180.9\t Episode return: -243\t Total steps: 10947\n",
      "Avg return in last 50 episodes: -175.6\t Episode return: -101\t Total steps: 11049\n",
      "Avg return in last 50 episodes: -176.7\t Episode return: -222\t Total steps: 11272\n",
      "Avg return in last 50 episodes: -172.3\t Episode return: -96\t Total steps: 11369\n",
      "Avg return in last 50 episodes: -169.24\t Episode return: -158\t Total steps: 11528\n",
      "Avg return in last 50 episodes: -170.9\t Episode return: -234\t Total steps: 11763\n",
      "Avg return in last 50 episodes: -169.14\t Episode return: -131\t Total steps: 11895\n",
      "Avg return in last 50 episodes: -164.28\t Episode return: -103\t Total steps: 11999\n",
      "Avg return in last 50 episodes: -162.76\t Episode return: -147\t Total steps: 12147\n",
      "Avg return in last 50 episodes: -162.56\t Episode return: -210\t Total steps: 12358\n",
      "Avg return in last 50 episodes: -161.62\t Episode return: -163\t Total steps: 12522\n",
      "Avg return in last 50 episodes: -161.5\t Episode return: -199\t Total steps: 12722\n",
      "Avg return in last 50 episodes: -154.2\t Episode return: -135\t Total steps: 12858\n",
      "Avg return in last 50 episodes: -153.46\t Episode return: -171\t Total steps: 13030\n",
      "Avg return in last 50 episodes: -153.94\t Episode return: -141\t Total steps: 13172\n",
      "Avg return in last 50 episodes: -154.14\t Episode return: -146\t Total steps: 13319\n",
      "Avg return in last 50 episodes: -154.84\t Episode return: -142\t Total steps: 13462\n",
      "Avg return in last 50 episodes: -156.82\t Episode return: -264\t Total steps: 13727\n",
      "Avg return in last 50 episodes: -156.94\t Episode return: -116\t Total steps: 13844\n",
      "Avg return in last 50 episodes: -156.38\t Episode return: -125\t Total steps: 13970\n",
      "Avg return in last 50 episodes: -154.96\t Episode return: -258\t Total steps: 14229\n",
      "Avg return in last 50 episodes: -155.16\t Episode return: -101\t Total steps: 14331\n",
      "Avg return in last 50 episodes: -156.06\t Episode return: -148\t Total steps: 14480\n",
      "Avg return in last 50 episodes: -154.54\t Episode return: -84\t Total steps: 14565\n",
      "Avg return in last 50 episodes: -154.56\t Episode return: -154\t Total steps: 14720\n",
      "Avg return in last 50 episodes: -154.22\t Episode return: -100\t Total steps: 14821\n",
      "Avg return in last 50 episodes: -153.58\t Episode return: -90\t Total steps: 14912\n",
      "Avg return in last 50 episodes: -154.26\t Episode return: -174\t Total steps: 15087\n"
     ]
    }
   ],
   "source": [
    "// Training the same agent for a total of at least 15000 steps.\n",
    "train(15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final agent testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total steps: 103\t Total reward: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f73cd330aa440fb5460957139758d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Deterministic() = true;\n",
    "\n",
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment envTest(\"gym.kurg.org\", \"4040\", \"Acrobot-v1\");\n",
    "envTest.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {double(agent.Action().action)};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += envTest.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
