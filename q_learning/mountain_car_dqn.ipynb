{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://lab.mlpack.org/v2/gh/mlpack/examples/master?urlpath=lab%2Ftree%2Fq_learning%2Facrobot_dqn.ipynb)\n",
    "\n",
    "You can easily run this notebook at https://lab.mlpack.org/\n",
    "\n",
    "This notebook is shows how to get use 3-Step Double DQN with Prioritized Replay to train an agent to get high scores for the [Acrobot](https://gym.openai.com/envs/Acrobot-v1) environment. \n",
    "\n",
    "We make the agent train and test on OpenAI Gym toolkit's GUI interface provided through a distributed infrastructure (TCP API). More details can be found [here](https://github.com/zoq/gym_tcp_api).\n",
    "\n",
    "A video of the trained agent can be seen in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including necessary libraries and namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/core.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/methods/ann/ffn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_learning.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_networks/simple_dqn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/environment/env_type.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/training_config.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to run the agent on gym's environment (provided externally) for testing.\n",
    "#include <gym/environment.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to generate and display a video of the trained agent.\n",
    "#include \"xwidgets/ximage.hpp\"\n",
    "#include \"xwidgets/xvideo.hpp\"\n",
    "#include \"xwidgets/xaudio.hpp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::ann;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace ens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::rl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the state and action space.\n",
    "DiscreteActionEnv::State::dimension = 2;\n",
    "DiscreteActionEnv::Action::size = 3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the network.\n",
    "FFN<MeanSquaredError<>, GaussianInitialization> network(\n",
    "    MeanSquaredError<>(), GaussianInitialization(0, 0.001));\n",
    "network.Add<Linear<>>(DiscreteActionEnv::State::dimension, 128);\n",
    "network.Add<ReLULayer<>>();\n",
    "network.Add<Linear<>>(128, DiscreteActionEnv::Action::size);\n",
    "// Set up the network.\n",
    "SimpleDQN<> model(network);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the policy method.\n",
    "GreedyPolicy<DiscreteActionEnv> policy(1.0, 1000, 0.1, 0.99);\n",
    "RandomReplay<DiscreteActionEnv> replayMethod(32, 10000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up training configurations.\n",
    "TrainingConfig config;\n",
    "config.TargetNetworkSyncInterval() = 100;\n",
    "config.ExplorationSteps() = 200*10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up DQN agent.\n",
    "QLearning<DiscreteActionEnv, decltype(model), AdamUpdate, decltype(policy), decltype(replayMethod)>\n",
    "    agent(config, model, policy, replayMethod);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the gym training environment.\n",
    "gym::Environment env(\"gym.kurg.org\", \"4040\", \"MountainCar-v0\");\n",
    "\n",
    "// Initializing training variables.\n",
    "std::vector<double> returnList;\n",
    "size_t episodes = 0;\n",
    "bool converged = true;\n",
    "\n",
    "// The number of episode returns to keep track of.\n",
    "size_t consecutiveEpisodes = 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Function to train the agent on mlpack's own implementation of the CartPole environment.\n",
    "void train(const size_t numSteps)\n",
    "{\n",
    "  agent.Deterministic() = false;\n",
    "  std::cout << \"Training for \" << numSteps << \" steps.\" << std::endl;\n",
    "  while (agent.TotalSteps() < numSteps)\n",
    "  {\n",
    "    double episodeReturn = 0;\n",
    "    double adjustedEpisodeReturn = 0;\n",
    "    env.reset();\n",
    "    do\n",
    "    {\n",
    "      agent.State().Data() = env.observation;\n",
    "      agent.SelectAction();\n",
    "      arma::mat action = {double(agent.Action().action)};\n",
    "\n",
    "      env.step(action);\n",
    "      DiscreteActionEnv::State nextState;\n",
    "      nextState.Data() = env.observation;\n",
    "      \n",
    "      // Use an adjusted reward for task completion.\n",
    "      double adjustedReward = env.reward;\n",
    "      if (nextState.Data()[0] < -0.8)\n",
    "        adjustedReward += 0.5;\n",
    "\n",
    "      replayMethod.Store(agent.State(), agent.Action(), adjustedReward, nextState,\n",
    "          env.done, 0.99);\n",
    "      episodeReturn += env.reward;\n",
    "      adjustedEpisodeReturn += adjustedReward;\n",
    "      agent.TotalSteps()++;\n",
    "      if (agent.Deterministic() || agent.TotalSteps() < config.ExplorationSteps())\n",
    "        continue;\n",
    "      agent.TrainAgent();\n",
    "    } while (!env.done);\n",
    "    returnList.push_back(episodeReturn);\n",
    "    episodes += 1;\n",
    "\n",
    "    if (returnList.size() > consecutiveEpisodes)\n",
    "      returnList.erase(returnList.begin());\n",
    "        \n",
    "    double averageReturn = std::accumulate(returnList.begin(),\n",
    "                                           returnList.end(), 0.0) /\n",
    "                           returnList.size();\n",
    "    if(episodes % 1 == 0)\n",
    "    {\n",
    "      std::cout << \"Avg return in last \" << consecutiveEpisodes\n",
    "          << \" episodes: \" << averageReturn\n",
    "          << \"\\t Episode return: \" << episodeReturn\n",
    "          << \"\\t Adjusted return: \" << adjustedEpisodeReturn\n",
    "          << \"\\t Total steps: \" << agent.TotalSteps() << std::endl;\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let the training begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps.\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 1000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 1200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 1400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 1600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 1800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 2000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 2200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 2400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 2600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 2800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 3000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 3200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -193.5\t Total steps: 3400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 3600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 3800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 4000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 4200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 4400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -193.5\t Total steps: 4600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -193\t Total steps: 4800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 5000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 5200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 5400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 5600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -193\t Total steps: 5800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -193.5\t Total steps: 6000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -192\t Total steps: 6200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -197\t Total steps: 6400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 6600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 6800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 7000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -178.5\t Total steps: 7200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 7400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -195\t Total steps: 7600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 7800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -172.5\t Total steps: 8000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 8200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 8400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -177.5\t Total steps: 8600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -185.5\t Total steps: 8800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 9000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 9200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -198.5\t Total steps: 9400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -182\t Total steps: 9600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -181\t Total steps: 9800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 10000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -179.5\t Total steps: 10200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -193.5\t Total steps: 10400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -188\t Total steps: 10600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -185\t Total steps: 10800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 11000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -174\t Total steps: 11200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 11400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -176\t Total steps: 11600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -195\t Total steps: 11800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -179.5\t Total steps: 12000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -193\t Total steps: 12200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -188.5\t Total steps: 12400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 12600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -194.5\t Total steps: 12800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -183.5\t Total steps: 13000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 13200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -196.5\t Total steps: 13400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -182.5\t Total steps: 13600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -185.5\t Total steps: 13800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 14000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -189.5\t Total steps: 14200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 14400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -199\t Total steps: 14600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 14800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -170.5\t Total steps: 15000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 15200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 15400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -186\t Total steps: 15600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -176.5\t Total steps: 15800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 16000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -189.5\t Total steps: 16200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 16400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -193.5\t Total steps: 16600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 16800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 17000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -188.5\t Total steps: 17200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -190.5\t Total steps: 17400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 17600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -189.5\t Total steps: 17800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -171\t Total steps: 18000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -185\t Total steps: 18200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -199\t Total steps: 18400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -194.5\t Total steps: 18600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 18800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -195.5\t Total steps: 19000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -182.5\t Total steps: 19200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -194.5\t Total steps: 19400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -189\t Total steps: 19600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -178\t Total steps: 19800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -195\t Total steps: 20000\n"
     ]
    }
   ],
   "source": [
    "// Training the agent for a total of at least 100 episodes.\n",
    "train(200*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total steps: 200\t Total reward: -200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daefcfc1f99b474eae344be4163e0320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Deterministic() = true;\n",
    "\n",
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment envTest(\"gym.kurg.org\", \"4040\", \"MountainCar-v0\");\n",
    "envTest.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {double(agent.Action().action)};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += envTest.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little more training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 60000 steps.\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -192.5\t Total steps: 20200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 20400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -190\t Total steps: 20600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -186\t Total steps: 20800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 21000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 21200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -180.5\t Total steps: 21400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -192.5\t Total steps: 21600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -175\t Total steps: 21800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -182\t Total steps: 22000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -191\t Total steps: 22200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 22400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -178.5\t Total steps: 22600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -191.5\t Total steps: 22800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -190.5\t Total steps: 23000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -196\t Total steps: 23200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -193\t Total steps: 23400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 23600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -177\t Total steps: 23800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -187.5\t Total steps: 24000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -195.5\t Total steps: 24200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -175.5\t Total steps: 24400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 24600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 24800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -176.5\t Total steps: 25000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 25200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 25400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -179.5\t Total steps: 25600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -188\t Total steps: 25800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 26000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -175.5\t Total steps: 26200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -180\t Total steps: 26400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -191.5\t Total steps: 26600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -179.5\t Total steps: 26800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 27000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -191\t Total steps: 27200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -195.5\t Total steps: 27400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -170.5\t Total steps: 27600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -178\t Total steps: 27800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 28000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -196.5\t Total steps: 28200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 28400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -198.5\t Total steps: 28600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -189.5\t Total steps: 28800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -189\t Total steps: 29000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -192.5\t Total steps: 29200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -192\t Total steps: 29400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 29600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -175\t Total steps: 29800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -170.5\t Total steps: 30000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 30200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 30400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 30600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 30800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -173.5\t Total steps: 31000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -196\t Total steps: 31200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 31400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -193\t Total steps: 31600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -177\t Total steps: 31800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -199\t Total steps: 32000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 32200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -187\t Total steps: 32400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -199\t Total steps: 32600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -187.5\t Total steps: 32800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -187\t Total steps: 33000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 33200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 33400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -196.5\t Total steps: 33600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 33800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -172\t Total steps: 34000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -194.5\t Total steps: 34200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 34400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -185.5\t Total steps: 34600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 34800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -188\t Total steps: 35000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 35200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 35400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -178.5\t Total steps: 35600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -181\t Total steps: 35800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -184\t Total steps: 36000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -192\t Total steps: 36200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 36400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 36600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 36800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -191\t Total steps: 37000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -187.5\t Total steps: 37200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -191.5\t Total steps: 37400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 37600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -186.5\t Total steps: 37800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -196.5\t Total steps: 38000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -192.5\t Total steps: 38200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -190.5\t Total steps: 38400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 38600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -194.5\t Total steps: 38800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -189.5\t Total steps: 39000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -182.5\t Total steps: 39200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 39400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -178\t Total steps: 39600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 39800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 40000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 40200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -190.5\t Total steps: 40400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -191\t Total steps: 40600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 40800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -176\t Total steps: 41000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -183\t Total steps: 41200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 41400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -195\t Total steps: 41600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -182\t Total steps: 41800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -188.5\t Total steps: 42000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -192\t Total steps: 42200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -186.5\t Total steps: 42400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 42600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -191.5\t Total steps: 42800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -194\t Total steps: 43000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 43200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 43400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -190.5\t Total steps: 43600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 43800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -184\t Total steps: 44000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 44200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 44400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 44600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -188.5\t Total steps: 44800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -174\t Total steps: 45000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 45200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 45400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 45600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -189.5\t Total steps: 45800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -186\t Total steps: 46000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -191.5\t Total steps: 46200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -194.5\t Total steps: 46400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -185.5\t Total steps: 46600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -176.5\t Total steps: 46800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -196\t Total steps: 47000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -192.5\t Total steps: 47200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -191\t Total steps: 47400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 47600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -192\t Total steps: 47800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -169.5\t Total steps: 48000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -173\t Total steps: 48200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -198\t Total steps: 48400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 48600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -189.5\t Total steps: 48800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -191.5\t Total steps: 49000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 49200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 49400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -175.5\t Total steps: 49600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -175\t Total steps: 49800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 50000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 50200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 50400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -180.5\t Total steps: 50600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -194.5\t Total steps: 50800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -180\t Total steps: 51000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -187.5\t Total steps: 51200\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -193\t Total steps: 51400\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 51600\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -190\t Total steps: 51800\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -194.5\t Total steps: 52000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg return in last 50 episodes: -199.98\t Episode return: -199\t Adjusted return: -187.5\t Total steps: 52199\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -200\t Total steps: 52399\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -200\t Total steps: 52599\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -189.5\t Total steps: 52799\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -200\t Total steps: 52999\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -172\t Total steps: 53199\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -200\t Total steps: 53399\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -188\t Total steps: 53599\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -190.5\t Total steps: 53799\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -188\t Total steps: 53999\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -200\t Total steps: 54199\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -188.5\t Total steps: 54399\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -180.5\t Total steps: 54599\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -200\t Total steps: 54799\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -193\t Total steps: 54999\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -187\t Total steps: 55199\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -198\t Total steps: 55399\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -188.5\t Total steps: 55599\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -200\t Total steps: 55799\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -197.5\t Total steps: 55999\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -200\t Total steps: 56199\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -192.5\t Total steps: 56399\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -181.5\t Total steps: 56599\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -200\t Total steps: 56799\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -200\t Total steps: 56999\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -187\t Total steps: 57199\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -200\t Total steps: 57399\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -200\t Total steps: 57599\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -195\t Total steps: 57799\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -200\t Total steps: 57999\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -189.5\t Total steps: 58199\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -189\t Total steps: 58399\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -193.5\t Total steps: 58599\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -200\t Total steps: 58799\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -187.5\t Total steps: 58999\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -187.5\t Total steps: 59199\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -200\t Total steps: 59399\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -198\t Total steps: 59599\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -190\t Total steps: 59799\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -200\t Total steps: 59999\n",
      "Avg return in last 50 episodes: -199.98\t Episode return: -200\t Adjusted return: -200\t Total steps: 60199\n"
     ]
    }
   ],
   "source": [
    "// Training the same agent for a total of at least 300 episodes.\n",
    "train(200*300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final agent testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total steps: 200\t Total reward: -200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9080f698634a3c8ad472df07f5c094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Deterministic() = true;\n",
    "\n",
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment envTest(\"gym.kurg.org\", \"4040\", \"MountainCar-v0\");\n",
    "envTest.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {double(agent.Action().action)};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += envTest.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
