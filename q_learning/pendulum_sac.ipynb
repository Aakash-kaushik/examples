{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://lab.mlpack.org/v2/gh/mlpack/examples/master?urlpath=lab%2Ftree%2Fq_learning%2Fpendulum_sac.ipynb)\n",
    "\n",
    "You can easily run this notebook at https://lab.mlpack.org/\n",
    "\n",
    "Here, we train a [Soft Actor-Critic](https://arxiv.org/abs/1801.01290) agent to get high scores for the [Pendulum](https://gym.openai.com/envs/Pendulum-v0/) environment. \n",
    "\n",
    "We make the agent train and test on OpenAI Gym toolkit's GUI interface provided through a distributed infrastructure (TCP API). More details can be found [here](https://github.com/zoq/gym_tcp_api).\n",
    "\n",
    "A video of the trained agent can be seen in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including necessary libraries and namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/core.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/methods/ann/ffn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/sac.hpp>\n",
    "#include <mlpack/methods/ann/loss_functions/empty_loss.hpp>\n",
    "#include <mlpack/methods/ann/init_rules/gaussian_init.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/environment/env_type.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/training_config.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to run the agent on gym's environment (provided externally) for testing.\n",
    "#include <gym/environment.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to generate and display a video of the trained agent.\n",
    "#include \"xwidgets/ximage.hpp\"\n",
    "#include \"xwidgets/xvideo.hpp\"\n",
    "#include \"xwidgets/xaudio.hpp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::ann;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace ens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::rl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the state and action space.\n",
    "ContinuousActionEnv::State::dimension = 3;\n",
    "ContinuousActionEnv::Action::size = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the actor and critic networks.\n",
    "FFN<EmptyLoss<>, GaussianInitialization>\n",
    "    policyNetwork(EmptyLoss<>(), GaussianInitialization(0, 0.1));\n",
    "policyNetwork.Add(new Linear<>(ContinuousActionEnv::State::dimension, 32));\n",
    "policyNetwork.Add(new ReLULayer<>());\n",
    "policyNetwork.Add(new Linear<>(32, ContinuousActionEnv::Action::size));\n",
    "policyNetwork.Add(new TanHLayer<>());\n",
    "\n",
    "FFN<EmptyLoss<>, GaussianInitialization>\n",
    "    qNetwork(EmptyLoss<>(), GaussianInitialization(0, 0.1));\n",
    "qNetwork.Add(new Linear<>(ContinuousActionEnv::State::dimension +\n",
    "                          ContinuousActionEnv::Action::size, 32));\n",
    "qNetwork.Add(new ReLULayer<>());\n",
    "qNetwork.Add(new Linear<>(32, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the policy method.\n",
    "RandomReplay<ContinuousActionEnv> replayMethod(32, 10000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up training configurations.\n",
    "TrainingConfig config;\n",
    "config.TargetNetworkSyncInterval() = 1;\n",
    "config.UpdateInterval() = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up Soft actor-critic agent.\n",
    "SAC<ContinuousActionEnv, decltype(qNetwork), decltype(policyNetwork), AdamUpdate>\n",
    "    agent(config, qNetwork, policyNetwork, replayMethod);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the gym training environment.\n",
    "gym::Environment env(\"gym.kurg.org\", \"4040\", \"Pendulum-v0\");\n",
    "\n",
    "// Initializing training variables.\n",
    "std::vector<double> returnList;\n",
    "size_t episodes = 0;\n",
    "bool converged = true;\n",
    "\n",
    "// The number of episode returns to keep track of.\n",
    "size_t consecutiveEpisodes = 25;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Function to train the agent on the Pendulum gym environment.\n",
    "void train(const size_t numSteps)\n",
    "{\n",
    "  agent.Deterministic() = false;\n",
    "  std::cout << \"Training for \" << numSteps << \" steps.\" << std::endl;\n",
    "  while (agent.TotalSteps() < numSteps)\n",
    "  {\n",
    "    double episodeReturn = 0;\n",
    "    env.reset();\n",
    "    do\n",
    "    {\n",
    "      agent.State().Data() = env.observation;\n",
    "      agent.SelectAction();\n",
    "      arma::mat action = {double(agent.Action().action[0] * 2)};\n",
    "\n",
    "      env.step(action);\n",
    "      ContinuousActionEnv::State nextState;\n",
    "      nextState.Data() = env.observation;\n",
    "\n",
    "      replayMethod.Store(agent.State(), agent.Action(), env.reward, nextState,\n",
    "          env.done, 0.99);\n",
    "      episodeReturn += env.reward;\n",
    "      agent.TotalSteps()++;\n",
    "      if (agent.Deterministic() || agent.TotalSteps() < config.ExplorationSteps())\n",
    "        continue;\n",
    "      for (size_t i = 0; i < config.UpdateInterval(); i++)\n",
    "        agent.Update();\n",
    "    } while (!env.done);\n",
    "    returnList.push_back(episodeReturn);\n",
    "    episodes += 1;\n",
    "\n",
    "    if (returnList.size() > consecutiveEpisodes)\n",
    "      returnList.erase(returnList.begin());\n",
    "        \n",
    "    double averageReturn = std::accumulate(returnList.begin(),\n",
    "                                           returnList.end(), 0.0) /\n",
    "                           returnList.size();\n",
    "    if(episodes % 2 == 0)\n",
    "    {\n",
    "      std::cout << \"Avg return in last \" << returnList.size()\n",
    "          << \" episodes: \" << averageReturn\n",
    "          << \"\\t Episode return: \" << episodeReturn\n",
    "          << \"\\t Total steps: \" << agent.TotalSteps() << std::endl;\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let the training begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps.\n",
      "Avg return in last 2 episodes: -1492.3\t Episode return: -1406.77\t Total steps: 400\n",
      "Avg return in last 4 episodes: -1193.68\t Episode return: -972.225\t Total steps: 800\n",
      "Avg return in last 6 episodes: -1193.38\t Episode return: -1112.53\t Total steps: 1200\n",
      "Avg return in last 8 episodes: -1258.5\t Episode return: -1598.63\t Total steps: 1600\n",
      "Avg return in last 10 episodes: -1219.42\t Episode return: -835.847\t Total steps: 2000\n",
      "Avg return in last 12 episodes: -1199.54\t Episode return: -1108.56\t Total steps: 2400\n",
      "Avg return in last 14 episodes: -1133.13\t Episode return: -800.636\t Total steps: 2800\n",
      "Avg return in last 16 episodes: -1118.45\t Episode return: -1345.97\t Total steps: 3200\n",
      "Avg return in last 18 episodes: -1150.39\t Episode return: -1285.81\t Total steps: 3600\n",
      "Avg return in last 20 episodes: -1131.89\t Episode return: -1027.84\t Total steps: 4000\n",
      "Avg return in last 22 episodes: -1145.8\t Episode return: -1433.4\t Total steps: 4400\n",
      "Avg return in last 24 episodes: -1151.79\t Episode return: -1149.27\t Total steps: 4800\n"
     ]
    }
   ],
   "source": [
    "// Training the agent for a total of at least 5000 steps.\n",
    "train(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total steps: 200\t Total reward: -1206.9\n",
      "https://gym.kurg.org/92f59075e7084/output.webm"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47e845de4204c518affb9a56d67e8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Deterministic() = true;\n",
    "\n",
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment envTest(\"gym.kurg.org\", \"4040\", \"Pendulum-v0\");\n",
    "envTest.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {double(agent.Action().action[0] * 2)};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += envTest.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "std::cout << url;\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little more training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps.\n",
      "Avg return in last 25 episodes: -1147.37\t Episode return: -1161.32\t Total steps: 5200\n",
      "Avg return in last 25 episodes: -1124.22\t Episode return: -667.611\t Total steps: 5600\n",
      "Avg return in last 25 episodes: -1098.34\t Episode return: -635.474\t Total steps: 6000\n",
      "Avg return in last 25 episodes: -1090.19\t Episode return: -770.662\t Total steps: 6400\n",
      "Avg return in last 25 episodes: -1083.9\t Episode return: -1527.91\t Total steps: 6800\n",
      "Avg return in last 25 episodes: -1086.25\t Episode return: -653.5\t Total steps: 7200\n",
      "Avg return in last 25 episodes: -1129.77\t Episode return: -1495.39\t Total steps: 7600\n",
      "Avg return in last 25 episodes: -1193.97\t Episode return: -1510.9\t Total steps: 8000\n",
      "Avg return in last 25 episodes: -1177.44\t Episode return: -1240.38\t Total steps: 8400\n",
      "Avg return in last 25 episodes: -1196.74\t Episode return: -1256.28\t Total steps: 8800\n",
      "Avg return in last 25 episodes: -1200.86\t Episode return: -1083.71\t Total steps: 9200\n",
      "Avg return in last 25 episodes: -1201.76\t Episode return: -1375.02\t Total steps: 9600\n",
      "Avg return in last 25 episodes: -1205.73\t Episode return: -1333.52\t Total steps: 10000\n",
      "Avg return in last 25 episodes: -1195.89\t Episode return: -1008.13\t Total steps: 10400\n",
      "Avg return in last 25 episodes: -1200.23\t Episode return: -899.832\t Total steps: 10800\n",
      "Avg return in last 25 episodes: -1193.52\t Episode return: -940.187\t Total steps: 11200\n",
      "Avg return in last 25 episodes: -1184.77\t Episode return: -841.744\t Total steps: 11600\n",
      "Avg return in last 25 episodes: -1153.22\t Episode return: -885.769\t Total steps: 12000\n",
      "Avg return in last 25 episodes: -1136.14\t Episode return: -636.275\t Total steps: 12400\n",
      "Avg return in last 25 episodes: -1074.35\t Episode return: -746.145\t Total steps: 12800\n",
      "Avg return in last 25 episodes: -1029.47\t Episode return: -858.087\t Total steps: 13200\n",
      "Avg return in last 25 episodes: -970.553\t Episode return: -505.745\t Total steps: 13600\n",
      "Avg return in last 25 episodes: -923.435\t Episode return: -630.844\t Total steps: 14000\n",
      "Avg return in last 25 episodes: -892.34\t Episode return: -819.185\t Total steps: 14400\n",
      "Avg return in last 25 episodes: -844.834\t Episode return: -829.466\t Total steps: 14800\n",
      "Avg return in last 25 episodes: -816.606\t Episode return: -762.541\t Total steps: 15200\n",
      "Avg return in last 25 episodes: -814.787\t Episode return: -892.732\t Total steps: 15600\n",
      "Avg return in last 25 episodes: -821.625\t Episode return: -1018.02\t Total steps: 16000\n",
      "Avg return in last 25 episodes: -784.606\t Episode return: -240.661\t Total steps: 16400\n",
      "Avg return in last 25 episodes: -714.057\t Episode return: -135.617\t Total steps: 16800\n",
      "Avg return in last 25 episodes: -655.785\t Episode return: -128.84\t Total steps: 17200\n",
      "Avg return in last 25 episodes: -614.175\t Episode return: -248.688\t Total steps: 17600\n",
      "Avg return in last 25 episodes: -560.077\t Episode return: -140.469\t Total steps: 18000\n",
      "Avg return in last 25 episodes: -518.913\t Episode return: -131.412\t Total steps: 18400\n",
      "Avg return in last 25 episodes: -533.61\t Episode return: -490.479\t Total steps: 18800\n",
      "Avg return in last 25 episodes: -520.326\t Episode return: -537.352\t Total steps: 19200\n",
      "Avg return in last 25 episodes: -505.096\t Episode return: -770.409\t Total steps: 19600\n",
      "Avg return in last 25 episodes: -504.405\t Episode return: -1112.26\t Total steps: 20000\n"
     ]
    }
   ],
   "source": [
    "// Training the same agent for a total of at least 20000 steps.\n",
    "train(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final agent testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total steps: 200\t Total reward: -288.541\n",
      "https://gym.kurg.org/6e74c868e2284/output.webm"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e9ffa28f494fb487f5fa04f1e0ed49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Deterministic() = true;\n",
    "\n",
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment envTest(\"gym.kurg.org\", \"4040\", \"Pendulum-v0\");\n",
    "envTest.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {double(agent.Action().action[0] * 2)};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += envTest.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "std::cout << url;\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
