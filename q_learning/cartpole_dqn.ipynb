{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// You can easily run this notebook at https://lab.mlpack.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/core.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/methods/ann/ffn.hpp>\n",
    "#include <mlpack/methods/ann/init_rules/gaussian_init.hpp>\n",
    "#include <mlpack/methods/ann/layer/layer.hpp>\n",
    "#include <mlpack/methods/ann/loss_functions/mean_squared_error.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_learning.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_networks/simple_dqn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/environment/mountain_car.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/environment/acrobot.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/environment/cart_pole.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/environment/double_pole_cart.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/training_config.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <ensmallen.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::ann;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace ens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::rl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the network.\n",
    "  SimpleDQN<> model(4, 256, 128, 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the policy and replay method.\n",
    "GreedyPolicy<CartPole> policy(1.0, 1000, 0.1, 0.99);\n",
    "RandomReplay<CartPole> replayMethod(20, 10000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingConfig config;\n",
    "config.StepSize() = 0.01;\n",
    "config.Discount() = 0.9;\n",
    "config.TargetNetworkSyncInterval() = 100;\n",
    "config.ExplorationSteps() = 100;\n",
    "config.DoubleQLearning() = false;\n",
    "config.StepLimit() = 200;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up DQN agent.\n",
    "QLearning<CartPole, decltype(model), AdamUpdate, decltype(policy)>\n",
    "  agent(std::move(config), std::move(model), std::move(policy),\n",
    "  std::move(replayMethod));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return: 25 Episode return: 25\n",
      "Average return: 16.5 Episode return: 8\n",
      "Average return: 18 Episode return: 21\n",
      "Average return: 20.5 Episode return: 28\n",
      "Average return: 19.4 Episode return: 15\n",
      "Average return: 19 Episode return: 17\n",
      "Average return: 17.8571 Episode return: 11\n",
      "Average return: 16.75 Episode return: 9\n",
      "Average return: 16.2222 Episode return: 12\n",
      "Average return: 24.9 Episode return: 103\n",
      "Average return: 24 Episode return: 15\n",
      "Average return: 23.1667 Episode return: 14\n",
      "Average return: 22.2308 Episode return: 11\n",
      "Average return: 22.1429 Episode return: 21\n",
      "Average return: 22.2667 Episode return: 24\n",
      "Average return: 21.875 Episode return: 16\n",
      "Average return: 21.9412 Episode return: 23\n",
      "Average return: 22 Episode return: 23\n",
      "Average return: 21.4211 Episode return: 11\n",
      "Average return: 20.95 Episode return: 12\n",
      "Average return: 20.9048 Episode return: 20\n",
      "Average return: 20.4545 Episode return: 11\n",
      "Average return: 20.2609 Episode return: 16\n",
      "Average return: 20.25 Episode return: 20\n",
      "Average return: 20.04 Episode return: 15\n",
      "Average return: 19.9615 Episode return: 18\n",
      "Average return: 20.9259 Episode return: 46\n",
      "Average return: 23.8571 Episode return: 103\n",
      "Average return: 26.5517 Episode return: 102\n",
      "Average return: 26.5 Episode return: 25\n",
      "Average return: 28.9032 Episode return: 101\n",
      "Average return: 32.5312 Episode return: 145\n",
      "Average return: 33.7879 Episode return: 74\n",
      "Average return: 35.8824 Episode return: 105\n",
      "Average return: 38.8571 Episode return: 140\n",
      "Average return: 42.3056 Episode return: 163\n",
      "Average return: 44.4324 Episode return: 121\n",
      "Average return: 48.5263 Episode return: 200\n",
      "Average return: 52.4103 Episode return: 200\n",
      "Average return in deterministic test: 199\n"
     ]
    }
   ],
   "source": [
    "// Training the agent on local environment:\n",
    "\n",
    "arma::running_stat<double> averageReturn;\n",
    "size_t episodes = 0;\n",
    "bool converged = true;\n",
    "\n",
    "while (true)\n",
    "{\n",
    "    double episodeReturn = agent.Episode();\n",
    "    averageReturn(episodeReturn);\n",
    "    episodes += 1;\n",
    "    \n",
    "    std::cout << \"Average return: \" << averageReturn.mean()\n",
    "        << \" Episode return: \" << episodeReturn << std::endl;\n",
    "    \n",
    "    if (episodes > 1000)\n",
    "    {\n",
    "      std::cout << \"Cart Pole with DQN failed.\" << std::endl;\n",
    "      converged = false;\n",
    "      break;\n",
    "    }\n",
    "    \n",
    "    if (averageReturn.mean() > 50)\n",
    "    {\n",
    "      agent.Deterministic() = true;\n",
    "      arma::running_stat<double> testReturn;\n",
    "      for (size_t i = 0; i < 10; ++i)\n",
    "        testReturn(agent.Episode());\n",
    "\n",
    "      std::cout << \"Average return in deterministic test: \"\n",
    "          << testReturn.mean() << std::endl;\n",
    "      break;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <gym/environment.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include \"xwidgets/ximage.hpp\"\n",
    "#include \"xwidgets/xvideo.hpp\"\n",
    "#include \"xwidgets/xaudio.hpp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym::Environment env(\"gym.kurg.org\", \"4040\", \"CartPole-v0\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.monitor.start(\"./dummy/\", true, true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@0x7f85c5e95530"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:         0\n",
      " Current step: 1 current reward: 1\n",
      "action:         0\n",
      " Current step: 2 current reward: 2\n",
      "action:    1.0000\n",
      " Current step: 3 current reward: 3\n",
      "action:         0\n",
      " Current step: 4 current reward: 4\n",
      "action:    1.0000\n",
      " Current step: 5 current reward: 5\n",
      "action:    1.0000\n",
      " Current step: 6 current reward: 6\n",
      "action:         0\n",
      " Current step: 7 current reward: 7\n",
      "action:    1.0000\n",
      " Current step: 8 current reward: 8\n",
      "action:         0\n",
      " Current step: 9 current reward: 9\n",
      "action:    1.0000\n",
      " Current step: 10 current reward: 10\n",
      "action:         0\n",
      " Current step: 11 current reward: 11\n",
      "action:    1.0000\n",
      " Current step: 12 current reward: 12\n",
      "action:    1.0000\n",
      " Current step: 13 current reward: 13\n",
      "action:         0\n",
      " Current step: 14 current reward: 14\n",
      "action:         0\n",
      " Current step: 15 current reward: 15\n",
      "action:    1.0000\n",
      " Current step: 16 current reward: 16\n",
      "action:    1.0000\n",
      " Current step: 17 current reward: 17\n",
      "action:         0\n",
      " Current step: 18 current reward: 18\n",
      "action:    1.0000\n",
      " Current step: 19 current reward: 19\n",
      "action:         0\n",
      " Current step: 20 current reward: 20\n",
      "action:    1.0000\n",
      " Current step: 21 current reward: 21\n",
      "action:         0\n",
      " Current step: 22 current reward: 22\n",
      "action:    1.0000\n",
      " Current step: 23 current reward: 23\n",
      "action:         0\n",
      " Current step: 24 current reward: 24\n",
      "action:    1.0000\n",
      " Current step: 25 current reward: 25\n",
      "action:         0\n",
      " Current step: 26 current reward: 26\n",
      "action:    1.0000\n",
      " Current step: 27 current reward: 27\n",
      "action:         0\n",
      " Current step: 28 current reward: 28\n",
      "action:    1.0000\n",
      " Current step: 29 current reward: 29\n",
      "action:         0\n",
      " Current step: 30 current reward: 30\n",
      "action:    1.0000\n",
      " Current step: 31 current reward: 31\n",
      "action:         0\n",
      " Current step: 32 current reward: 32\n",
      "action:    1.0000\n",
      " Current step: 33 current reward: 33\n",
      "action:         0\n",
      " Current step: 34 current reward: 34\n",
      "action:    1.0000\n",
      " Current step: 35 current reward: 35\n",
      "action:         0\n",
      " Current step: 36 current reward: 36\n",
      "action:    1.0000\n",
      " Current step: 37 current reward: 37\n",
      "action:         0\n",
      " Current step: 38 current reward: 38\n",
      "action:    1.0000\n",
      " Current step: 39 current reward: 39\n",
      "action:         0\n",
      " Current step: 40 current reward: 40\n",
      "action:    1.0000\n",
      " Current step: 41 current reward: 41\n",
      "action:         0\n",
      " Current step: 42 current reward: 42\n",
      "action:    1.0000\n",
      " Current step: 43 current reward: 43\n",
      "action:         0\n",
      " Current step: 44 current reward: 44\n",
      "action:    1.0000\n",
      " Current step: 45 current reward: 45\n",
      "action:         0\n",
      " Current step: 46 current reward: 46\n",
      "action:    1.0000\n",
      " Current step: 47 current reward: 47\n",
      "action:         0\n",
      " Current step: 48 current reward: 48\n",
      "action:    1.0000\n",
      " Current step: 49 current reward: 49\n",
      "action:         0\n",
      " Current step: 50 current reward: 50\n",
      "action:    1.0000\n",
      " Current step: 51 current reward: 51\n",
      "action:         0\n",
      " Current step: 52 current reward: 52\n",
      "action:    1.0000\n",
      " Current step: 53 current reward: 53\n",
      "action:         0\n",
      " Current step: 54 current reward: 54\n",
      "action:    1.0000\n",
      " Current step: 55 current reward: 55\n",
      "action:         0\n",
      " Current step: 56 current reward: 56\n",
      "action:    1.0000\n",
      " Current step: 57 current reward: 57\n",
      "action:         0\n",
      " Current step: 58 current reward: 58\n",
      "action:    1.0000\n",
      " Current step: 59 current reward: 59\n",
      "action:         0\n",
      " Current step: 60 current reward: 60\n",
      "action:    1.0000\n",
      " Current step: 61 current reward: 61\n",
      "action:         0\n",
      " Current step: 62 current reward: 62\n",
      "action:    1.0000\n",
      " Current step: 63 current reward: 63\n",
      "action:    1.0000\n",
      " Current step: 64 current reward: 64\n",
      "action:         0\n",
      " Current step: 65 current reward: 65\n",
      "action:    1.0000\n",
      " Current step: 66 current reward: 66\n",
      "action:         0\n",
      " Current step: 67 current reward: 67\n",
      "action:         0\n",
      " Current step: 68 current reward: 68\n",
      "action:    1.0000\n",
      " Current step: 69 current reward: 69\n",
      "action:    1.0000\n",
      " Current step: 70 current reward: 70\n",
      "action:         0\n",
      " Current step: 71 current reward: 71\n",
      "action:         0\n",
      " Current step: 72 current reward: 72\n",
      "action:    1.0000\n",
      " Current step: 73 current reward: 73\n",
      "action:    1.0000\n",
      " Current step: 74 current reward: 74\n",
      "action:         0\n",
      " Current step: 75 current reward: 75\n",
      "action:         0\n",
      " Current step: 76 current reward: 76\n",
      "action:    1.0000\n",
      " Current step: 77 current reward: 77\n",
      "action:    1.0000\n",
      " Current step: 78 current reward: 78\n",
      "action:         0\n",
      " Current step: 79 current reward: 79\n",
      "action:    1.0000\n",
      " Current step: 80 current reward: 80\n",
      "action:         0\n",
      " Current step: 81 current reward: 81\n",
      "action:         0\n",
      " Current step: 82 current reward: 82\n",
      "action:    1.0000\n",
      " Current step: 83 current reward: 83\n",
      "action:    1.0000\n",
      " Current step: 84 current reward: 84\n",
      "action:         0\n",
      " Current step: 85 current reward: 85\n",
      "action:    1.0000\n",
      " Current step: 86 current reward: 86\n",
      "action:         0\n",
      " Current step: 87 current reward: 87\n",
      "action:    1.0000\n",
      " Current step: 88 current reward: 88\n",
      "action:         0\n",
      " Current step: 89 current reward: 89\n",
      "action:    1.0000\n",
      " Current step: 90 current reward: 90\n",
      "action:         0\n",
      " Current step: 91 current reward: 91\n",
      "action:    1.0000\n",
      " Current step: 92 current reward: 92\n",
      "action:         0\n",
      " Current step: 93 current reward: 93\n",
      "action:         0\n",
      " Current step: 94 current reward: 94\n",
      "action:    1.0000\n",
      " Current step: 95 current reward: 95\n",
      "action:    1.0000\n",
      " Current step: 96 current reward: 96\n",
      "action:         0\n",
      " Current step: 97 current reward: 97\n",
      "action:    1.0000\n",
      " Current step: 98 current reward: 98\n",
      "action:         0\n",
      " Current step: 99 current reward: 99\n",
      "action:    1.0000\n",
      " Current step: 100 current reward: 100\n",
      "action:         0\n",
      " Current step: 101 current reward: 101\n",
      "action:    1.0000\n",
      " Current step: 102 current reward: 102\n",
      "action:         0\n",
      " Current step: 103 current reward: 103\n",
      "action:    1.0000\n",
      " Current step: 104 current reward: 104\n",
      "action:         0\n",
      " Current step: 105 current reward: 105\n",
      "action:    1.0000\n",
      " Current step: 106 current reward: 106\n",
      "action:         0\n",
      " Current step: 107 current reward: 107\n",
      "action:    1.0000\n",
      " Current step: 108 current reward: 108\n",
      "action:         0\n",
      " Current step: 109 current reward: 109\n",
      "action:    1.0000\n",
      " Current step: 110 current reward: 110\n",
      "action:         0\n",
      " Current step: 111 current reward: 111\n",
      "action:    1.0000\n",
      " Current step: 112 current reward: 112\n",
      "action:         0\n",
      " Current step: 113 current reward: 113\n",
      "action:    1.0000\n",
      " Current step: 114 current reward: 114\n",
      "action:    1.0000\n",
      " Current step: 115 current reward: 115\n",
      "action:         0\n",
      " Current step: 116 current reward: 116\n",
      "action:         0\n",
      " Current step: 117 current reward: 117\n",
      "action:    1.0000\n",
      " Current step: 118 current reward: 118\n",
      "action:    1.0000\n",
      " Current step: 119 current reward: 119\n",
      "action:         0\n",
      " Current step: 120 current reward: 120\n",
      "action:         0\n",
      " Current step: 121 current reward: 121\n",
      "action:    1.0000\n",
      " Current step: 122 current reward: 122\n",
      "action:    1.0000\n",
      " Current step: 123 current reward: 123\n",
      "action:         0\n",
      " Current step: 124 current reward: 124\n",
      "action:    1.0000\n",
      " Current step: 125 current reward: 125\n",
      "action:         0\n",
      " Current step: 126 current reward: 126\n",
      "action:         0\n",
      " Current step: 127 current reward: 127\n",
      "action:    1.0000\n",
      " Current step: 128 current reward: 128\n",
      "action:    1.0000\n",
      " Current step: 129 current reward: 129\n",
      "action:         0\n",
      " Current step: 130 current reward: 130\n",
      "action:    1.0000\n",
      " Current step: 131 current reward: 131\n",
      "action:         0\n",
      " Current step: 132 current reward: 132\n",
      "action:    1.0000\n",
      " Current step: 133 current reward: 133\n",
      "action:         0\n",
      " Current step: 134 current reward: 134\n",
      "action:         0\n",
      " Current step: 135 current reward: 135\n",
      "action:    1.0000\n",
      " Current step: 136 current reward: 136\n",
      "action:    1.0000\n",
      " Current step: 137 current reward: 137\n",
      "action:         0\n",
      " Current step: 138 current reward: 138\n",
      "action:    1.0000\n",
      " Current step: 139 current reward: 139\n",
      "action:         0\n",
      " Current step: 140 current reward: 140\n",
      "action:    1.0000\n",
      " Current step: 141 current reward: 141\n",
      "action:         0\n",
      " Current step: 142 current reward: 142\n",
      "action:    1.0000\n",
      " Current step: 143 current reward: 143\n",
      "action:         0\n",
      " Current step: 144 current reward: 144\n",
      "action:    1.0000\n",
      " Current step: 145 current reward: 145\n",
      "action:         0\n",
      " Current step: 146 current reward: 146\n",
      "action:    1.0000\n",
      " Current step: 147 current reward: 147\n",
      "action:         0\n",
      " Current step: 148 current reward: 148\n",
      "action:    1.0000\n",
      " Current step: 149 current reward: 149\n",
      "action:    1.0000\n",
      " Current step: 150 current reward: 150\n",
      "action:         0\n",
      " Current step: 151 current reward: 151\n",
      "action:         0\n",
      " Current step: 152 current reward: 152\n",
      "action:    1.0000\n",
      " Current step: 153 current reward: 153\n",
      "action:    1.0000\n",
      " Current step: 154 current reward: 154\n",
      "action:         0\n",
      " Current step: 155 current reward: 155\n",
      "action:         0\n",
      " Current step: 156 current reward: 156\n",
      "action:    1.0000\n",
      " Current step: 157 current reward: 157\n",
      "action:    1.0000\n",
      " Current step: 158 current reward: 158\n",
      "action:         0\n",
      " Current step: 159 current reward: 159\n",
      "action:    1.0000\n",
      " Current step: 160 current reward: 160\n",
      "action:         0\n",
      " Current step: 161 current reward: 161\n",
      "action:    1.0000\n",
      " Current step: 162 current reward: 162\n",
      "action:         0\n",
      " Current step: 163 current reward: 163\n",
      "action:    1.0000\n",
      " Current step: 164 current reward: 164\n",
      "action:         0\n",
      " Current step: 165 current reward: 165\n",
      "action:    1.0000\n",
      " Current step: 166 current reward: 166\n",
      "action:         0\n",
      " Current step: 167 current reward: 167\n",
      "action:    1.0000\n",
      " Current step: 168 current reward: 168\n",
      "action:         0\n",
      " Current step: 169 current reward: 169\n",
      "action:    1.0000\n",
      " Current step: 170 current reward: 170\n",
      "action:         0\n",
      " Current step: 171 current reward: 171\n",
      "action:    1.0000\n",
      " Current step: 172 current reward: 172\n",
      "action:         0\n",
      " Current step: 173 current reward: 173\n",
      "action:    1.0000\n",
      " Current step: 174 current reward: 174\n",
      "action:         0\n",
      " Current step: 175 current reward: 175\n",
      "action:    1.0000\n",
      " Current step: 176 current reward: 176\n",
      "action:    1.0000\n",
      " Current step: 177 current reward: 177\n",
      "action:         0\n",
      " Current step: 178 current reward: 178\n",
      "action:    1.0000\n",
      " Current step: 179 current reward: 179\n",
      "action:         0\n",
      " Current step: 180 current reward: 180\n",
      "action:    1.0000\n",
      " Current step: 181 current reward: 181\n",
      "action:         0\n",
      " Current step: 182 current reward: 182\n",
      "action:    1.0000\n",
      " Current step: 183 current reward: 183\n",
      "action:         0\n",
      " Current step: 184 current reward: 184\n",
      "action:    1.0000\n",
      " Current step: 185 current reward: 185\n",
      "action:         0\n",
      " Current step: 186 current reward: 186\n",
      "action:    1.0000\n",
      " Current step: 187 current reward: 187\n",
      "action:         0\n",
      " Current step: 188 current reward: 188\n",
      "action:    1.0000\n",
      " Current step: 189 current reward: 189\n",
      "action:         0\n",
      " Current step: 190 current reward: 190\n",
      "action:    1.0000\n",
      " Current step: 191 current reward: 191\n",
      "action:         0\n",
      " Current step: 192 current reward: 192\n",
      "action:    1.0000\n",
      " Current step: 193 current reward: 193\n",
      "action:    1.0000\n",
      " Current step: 194 current reward: 194\n",
      "action:         0\n",
      " Current step: 195 current reward: 195\n",
      "action:    1.0000\n",
      " Current step: 196 current reward: 196\n",
      "action:         0\n",
      " Current step: 197 current reward: 197\n",
      "action:    1.0000\n",
      " Current step: 198 current reward: 198\n",
      "action:         0\n",
      " Current step: 199 current reward: 199\n",
      "action:    1.0000\n"
     ]
    }
   ],
   "source": [
    "// Testing the agent on gym's environment\n",
    "while (1)\n",
    "  {\n",
    "    // State from the environment is passed to the agent's internal representation\n",
    "    agent.State().Data() = env.observation;\n",
    "    \n",
    "    // with the given state, the agent performs an action according to its defined policy\n",
    "    agent.Step();\n",
    "    \n",
    "    // Action to take, decided by the policy\n",
    "    arma::mat action = {double(agent.Action())};\n",
    "\n",
    "    // arma::mat action = env.action_space.sample();\n",
    "    std::cout << \"action: \" << action;\n",
    "\n",
    "    env.step(action);\n",
    "    totalReward += env.reward;\n",
    "    totalSteps += 1;\n",
    "\n",
    "    if (env.done)\n",
    "      break;\n",
    "\n",
    "    std::cout << \" Current step: \" << totalSteps << \" current reward: \"\n",
    "              << totalReward << std::endl;\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://gym.kurg.org/647380835f904/output.webm\n"
     ]
    }
   ],
   "source": [
    "env.close();\n",
    "std::string url = env.url();\n",
    "std::cout << url << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583a7def6d89409eade00945859f389a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
