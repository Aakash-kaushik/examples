{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://lab.mlpack.org/v2/gh/mlpack/examples/master?urlpath=lab%2Ftree%2Fq_learning%2Fcartpole_dqn.ipynb)\n",
    "\n",
    "You can easily run this notebook at https://lab.mlpack.org/\n",
    "\n",
    "This notebook shows how to get started with training reinforcement learning agents, particularly DQN agents, using mlpack. Here, we train a [Simple DQN](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) agent to get high scores for the [CartPole](https://gym.openai.com/envs/CartPole-v0) environment. \n",
    "\n",
    "We make the agent train and test on OpenAI Gym toolkit's GUI interface provided through a distributed infrastructure (TCP API). More details can be found [here](https://github.com/zoq/gym_tcp_api).\n",
    "\n",
    "A video of the trained agent can be seen in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including necessary libraries and namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/core.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/methods/ann/ffn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_learning.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_networks/simple_dqn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/environment/env_type.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/training_config.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to run the agent on gym's environment (provided externally) for testing.\n",
    "#include <gym/environment.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to generate and display a video of the trained agent.\n",
    "#include \"xwidgets/ximage.hpp\"\n",
    "#include \"xwidgets/xvideo.hpp\"\n",
    "#include \"xwidgets/xaudio.hpp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::ann;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace ens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::rl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the state and action space.\n",
    "DiscreteActionEnv::State::dimension = 4;\n",
    "DiscreteActionEnv::Action::size = 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the network.\n",
    "SimpleDQN<> model(4, 128, 32, 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the policy and replay method.\n",
    "GreedyPolicy<DiscreteActionEnv> policy(1.0, 1000, 0.1, 0.99);\n",
    "RandomReplay<DiscreteActionEnv> replayMethod(32, 2000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up training configurations.\n",
    "TrainingConfig config;\n",
    "config.StepSize() = 0.001;\n",
    "config.Discount() = 0.99;\n",
    "config.TargetNetworkSyncInterval() = 100;\n",
    "config.ExplorationSteps() = 100;\n",
    "config.DoubleQLearning() = false;\n",
    "config.StepLimit() = 200;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up DQN agent.\n",
    "QLearning<DiscreteActionEnv, decltype(model), AdamUpdate, decltype(policy)>\n",
    "    agent(config, model, policy, replayMethod);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the gym training environment.\n",
    "gym::Environment env(\"gym.kurg.org\", \"4040\", \"CartPole-v0\");\n",
    "\n",
    "// Set up the gym testing environment.\n",
    "gym::Environment envTest(\"gym.kurg.org\", \"4040\", \"CartPole-v0\");\n",
    "// Start test env monitor.\n",
    "envTest.compression(9);\n",
    "envTest.monitor.start(\"./dummy/\", true, true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Initializing training variables.\n",
    "std::vector<double> returnList;\n",
    "size_t episodes = 0;\n",
    "bool converged = true;\n",
    "// The number of episode returns to keep track of.\n",
    "size_t consecutiveEpisodes = 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Function to train the agent on mlpack's own implementation of the CartPole environment.\n",
    "void train(const size_t numSteps)\n",
    "{\n",
    "  agent.Deterministic() = false;\n",
    "  std::cout << \"Training for \" << numSteps << \" steps.\" << std::endl;\n",
    "  while (agent.TotalSteps() < numSteps)\n",
    "  {\n",
    "    double episodeReturn = 0;\n",
    "    env.reset();\n",
    "    do\n",
    "    {\n",
    "      agent.State().Data() = env.observation;\n",
    "      agent.SelectAction();\n",
    "      arma::mat action = {double(agent.Action().action)};\n",
    "\n",
    "      env.step(action);\n",
    "      DiscreteActionEnv::State nextState;\n",
    "      nextState.Data() = env.observation;\n",
    "\n",
    "      replayMethod.Store(agent.State(), agent.Action(), env.reward, nextState,\n",
    "          env.done, 0.99);\n",
    "      episodeReturn += env.reward;\n",
    "      agent.TotalSteps()++;\n",
    "      if (agent.Deterministic() || agent.TotalSteps() < config.ExplorationSteps())\n",
    "        continue;\n",
    "      agent.TrainAgent();\n",
    "    } while (!env.done);\n",
    "    returnList.push_back(episodeReturn);\n",
    "    episodes += 1;\n",
    "\n",
    "    if (returnList.size() > consecutiveEpisodes)\n",
    "      returnList.erase(returnList.begin());\n",
    "        \n",
    "    double averageReturn = std::accumulate(returnList.begin(),\n",
    "                                           returnList.end(), 0.0) /\n",
    "                           returnList.size();\n",
    "    if(episodes % 1 == 0)\n",
    "    {\n",
    "      std::cout << \"Avg return in last \" << consecutiveEpisodes\n",
    "          << \" episodes: \" << averageReturn\n",
    "          << \"\\t Episode return: \" << episodeReturn\n",
    "          << \"\\t Total steps: \" << agent.TotalSteps() << std::endl;\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let the training begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2500 steps.\n",
      "Avg return in last 50 episodes: 16\t Episode return: 16\t Total steps: 16\n",
      "Avg return in last 50 episodes: 15\t Episode return: 14\t Total steps: 30\n",
      "Avg return in last 50 episodes: 16\t Episode return: 18\t Total steps: 48\n",
      "Avg return in last 50 episodes: 15\t Episode return: 12\t Total steps: 60\n",
      "Avg return in last 50 episodes: 15.2\t Episode return: 16\t Total steps: 76\n",
      "Avg return in last 50 episodes: 15.5\t Episode return: 17\t Total steps: 93\n",
      "Avg return in last 50 episodes: 17.7143\t Episode return: 31\t Total steps: 124\n",
      "Avg return in last 50 episodes: 17.625\t Episode return: 17\t Total steps: 141\n",
      "Avg return in last 50 episodes: 17\t Episode return: 12\t Total steps: 153\n",
      "Avg return in last 50 episodes: 16.5\t Episode return: 12\t Total steps: 165\n",
      "Avg return in last 50 episodes: 15.9091\t Episode return: 10\t Total steps: 175\n",
      "Avg return in last 50 episodes: 15.8333\t Episode return: 15\t Total steps: 190\n",
      "Avg return in last 50 episodes: 16.3846\t Episode return: 23\t Total steps: 213\n",
      "Avg return in last 50 episodes: 17.6429\t Episode return: 34\t Total steps: 247\n",
      "Avg return in last 50 episodes: 19.4667\t Episode return: 45\t Total steps: 292\n",
      "Avg return in last 50 episodes: 19.0625\t Episode return: 13\t Total steps: 305\n",
      "Avg return in last 50 episodes: 18.7647\t Episode return: 14\t Total steps: 319\n",
      "Avg return in last 50 episodes: 18.5556\t Episode return: 15\t Total steps: 334\n",
      "Avg return in last 50 episodes: 18.5789\t Episode return: 19\t Total steps: 353\n",
      "Avg return in last 50 episodes: 18.6\t Episode return: 19\t Total steps: 372\n",
      "Avg return in last 50 episodes: 18.5238\t Episode return: 17\t Total steps: 389\n",
      "Avg return in last 50 episodes: 18.3182\t Episode return: 14\t Total steps: 403\n",
      "Avg return in last 50 episodes: 18.0435\t Episode return: 12\t Total steps: 415\n",
      "Avg return in last 50 episodes: 17.9583\t Episode return: 16\t Total steps: 431\n",
      "Avg return in last 50 episodes: 18.04\t Episode return: 20\t Total steps: 451\n",
      "Avg return in last 50 episodes: 18.1923\t Episode return: 22\t Total steps: 473\n",
      "Avg return in last 50 episodes: 18.5185\t Episode return: 27\t Total steps: 500\n",
      "Avg return in last 50 episodes: 20.1429\t Episode return: 64\t Total steps: 564\n",
      "Avg return in last 50 episodes: 19.8276\t Episode return: 11\t Total steps: 575\n",
      "Avg return in last 50 episodes: 19.4667\t Episode return: 9\t Total steps: 584\n",
      "Avg return in last 50 episodes: 19.8387\t Episode return: 31\t Total steps: 615\n",
      "Avg return in last 50 episodes: 19.5625\t Episode return: 11\t Total steps: 626\n",
      "Avg return in last 50 episodes: 19.3333\t Episode return: 12\t Total steps: 638\n",
      "Avg return in last 50 episodes: 19.2059\t Episode return: 15\t Total steps: 653\n",
      "Avg return in last 50 episodes: 19.2286\t Episode return: 20\t Total steps: 673\n",
      "Avg return in last 50 episodes: 19.0556\t Episode return: 13\t Total steps: 686\n",
      "Avg return in last 50 episodes: 19.1622\t Episode return: 23\t Total steps: 709\n",
      "Avg return in last 50 episodes: 19.2105\t Episode return: 21\t Total steps: 730\n",
      "Avg return in last 50 episodes: 19.1538\t Episode return: 17\t Total steps: 747\n",
      "Avg return in last 50 episodes: 18.95\t Episode return: 11\t Total steps: 758\n",
      "Avg return in last 50 episodes: 19.2195\t Episode return: 30\t Total steps: 788\n",
      "Avg return in last 50 episodes: 19\t Episode return: 10\t Total steps: 798\n",
      "Avg return in last 50 episodes: 18.814\t Episode return: 11\t Total steps: 809\n",
      "Avg return in last 50 episodes: 18.5682\t Episode return: 8\t Total steps: 817\n",
      "Avg return in last 50 episodes: 18.6444\t Episode return: 22\t Total steps: 839\n",
      "Avg return in last 50 episodes: 18.5217\t Episode return: 13\t Total steps: 852\n",
      "Avg return in last 50 episodes: 18.383\t Episode return: 12\t Total steps: 864\n",
      "Avg return in last 50 episodes: 18.2083\t Episode return: 10\t Total steps: 874\n",
      "Avg return in last 50 episodes: 18.1224\t Episode return: 14\t Total steps: 888\n",
      "Avg return in last 50 episodes: 18.22\t Episode return: 23\t Total steps: 911\n",
      "Avg return in last 50 episodes: 18.2\t Episode return: 15\t Total steps: 926\n",
      "Avg return in last 50 episodes: 18.32\t Episode return: 20\t Total steps: 946\n",
      "Avg return in last 50 episodes: 18.2\t Episode return: 12\t Total steps: 958\n",
      "Avg return in last 50 episodes: 18.34\t Episode return: 19\t Total steps: 977\n",
      "Avg return in last 50 episodes: 18.32\t Episode return: 15\t Total steps: 992\n",
      "Avg return in last 50 episodes: 18.32\t Episode return: 17\t Total steps: 1009\n",
      "Avg return in last 50 episodes: 17.9\t Episode return: 10\t Total steps: 1019\n",
      "Avg return in last 50 episodes: 17.8\t Episode return: 12\t Total steps: 1031\n",
      "Avg return in last 50 episodes: 17.72\t Episode return: 8\t Total steps: 1039\n",
      "Avg return in last 50 episodes: 17.9\t Episode return: 21\t Total steps: 1060\n",
      "Avg return in last 50 episodes: 18.04\t Episode return: 17\t Total steps: 1077\n",
      "Avg return in last 50 episodes: 17.96\t Episode return: 11\t Total steps: 1088\n",
      "Avg return in last 50 episodes: 17.68\t Episode return: 9\t Total steps: 1097\n",
      "Avg return in last 50 episodes: 17.24\t Episode return: 12\t Total steps: 1109\n",
      "Avg return in last 50 episodes: 16.56\t Episode return: 11\t Total steps: 1120\n",
      "Avg return in last 50 episodes: 16.5\t Episode return: 10\t Total steps: 1130\n",
      "Avg return in last 50 episodes: 16.48\t Episode return: 13\t Total steps: 1143\n",
      "Avg return in last 50 episodes: 16.36\t Episode return: 9\t Total steps: 1152\n",
      "Avg return in last 50 episodes: 16.18\t Episode return: 10\t Total steps: 1162\n",
      "Avg return in last 50 episodes: 15.98\t Episode return: 9\t Total steps: 1171\n",
      "Avg return in last 50 episodes: 15.86\t Episode return: 11\t Total steps: 1182\n",
      "Avg return in last 50 episodes: 15.76\t Episode return: 9\t Total steps: 1191\n",
      "Avg return in last 50 episodes: 15.72\t Episode return: 10\t Total steps: 1201\n",
      "Avg return in last 50 episodes: 15.58\t Episode return: 9\t Total steps: 1210\n",
      "Avg return in last 50 episodes: 15.4\t Episode return: 11\t Total steps: 1221\n",
      "Avg return in last 50 episodes: 15.16\t Episode return: 10\t Total steps: 1231\n",
      "Avg return in last 50 episodes: 14.8\t Episode return: 9\t Total steps: 1240\n",
      "Avg return in last 50 episodes: 13.72\t Episode return: 10\t Total steps: 1250\n",
      "Avg return in last 50 episodes: 13.68\t Episode return: 9\t Total steps: 1259\n",
      "Avg return in last 50 episodes: 13.66\t Episode return: 8\t Total steps: 1267\n",
      "Avg return in last 50 episodes: 13.22\t Episode return: 9\t Total steps: 1276\n",
      "Avg return in last 50 episodes: 13.16\t Episode return: 8\t Total steps: 1284\n",
      "Avg return in last 50 episodes: 13.1\t Episode return: 9\t Total steps: 1293\n",
      "Avg return in last 50 episodes: 12.98\t Episode return: 9\t Total steps: 1302\n",
      "Avg return in last 50 episodes: 12.74\t Episode return: 8\t Total steps: 1310\n",
      "Avg return in last 50 episodes: 12.68\t Episode return: 10\t Total steps: 1320\n",
      "Avg return in last 50 episodes: 12.42\t Episode return: 10\t Total steps: 1330\n",
      "Avg return in last 50 episodes: 12.2\t Episode return: 10\t Total steps: 1340\n",
      "Avg return in last 50 episodes: 12.2\t Episode return: 17\t Total steps: 1357\n",
      "Avg return in last 50 episodes: 12.2\t Episode return: 11\t Total steps: 1368\n",
      "Avg return in last 50 episodes: 11.76\t Episode return: 8\t Total steps: 1376\n",
      "Avg return in last 50 episodes: 11.76\t Episode return: 10\t Total steps: 1386\n",
      "Avg return in last 50 episodes: 11.78\t Episode return: 12\t Total steps: 1398\n",
      "Avg return in last 50 episodes: 11.84\t Episode return: 11\t Total steps: 1409\n",
      "Avg return in last 50 episodes: 11.6\t Episode return: 10\t Total steps: 1419\n",
      "Avg return in last 50 episodes: 11.52\t Episode return: 9\t Total steps: 1428\n",
      "Avg return in last 50 episodes: 11.48\t Episode return: 10\t Total steps: 1438\n",
      "Avg return in last 50 episodes: 11.46\t Episode return: 9\t Total steps: 1447\n",
      "Avg return in last 50 episodes: 11.38\t Episode return: 10\t Total steps: 1457\n",
      "Avg return in last 50 episodes: 11.3\t Episode return: 19\t Total steps: 1476\n",
      "Avg return in last 50 episodes: 11.18\t Episode return: 9\t Total steps: 1485\n",
      "Avg return in last 50 episodes: 11.16\t Episode return: 19\t Total steps: 1504\n",
      "Avg return in last 50 episodes: 11.12\t Episode return: 10\t Total steps: 1514\n",
      "Avg return in last 50 episodes: 10.94\t Episode return: 10\t Total steps: 1524\n",
      "Avg return in last 50 episodes: 10.9\t Episode return: 13\t Total steps: 1537\n",
      "Avg return in last 50 episodes: 10.76\t Episode return: 10\t Total steps: 1547\n",
      "Avg return in last 50 episodes: 10.78\t Episode return: 11\t Total steps: 1558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg return in last 50 episodes: 11.1\t Episode return: 28\t Total steps: 1586\n",
      "Avg return in last 50 episodes: 11.12\t Episode return: 9\t Total steps: 1595\n",
      "Avg return in last 50 episodes: 10.9\t Episode return: 10\t Total steps: 1605\n",
      "Avg return in last 50 episodes: 10.84\t Episode return: 14\t Total steps: 1619\n",
      "Avg return in last 50 episodes: 10.78\t Episode return: 8\t Total steps: 1627\n",
      "Avg return in last 50 episodes: 10.78\t Episode return: 9\t Total steps: 1636\n",
      "Avg return in last 50 episodes: 10.72\t Episode return: 9\t Total steps: 1645\n",
      "Avg return in last 50 episodes: 11.46\t Episode return: 48\t Total steps: 1693\n",
      "Avg return in last 50 episodes: 11.46\t Episode return: 10\t Total steps: 1703\n",
      "Avg return in last 50 episodes: 11.46\t Episode return: 13\t Total steps: 1716\n",
      "Avg return in last 50 episodes: 12.58\t Episode return: 65\t Total steps: 1781\n",
      "Avg return in last 50 episodes: 13.38\t Episode return: 50\t Total steps: 1831\n",
      "Avg return in last 50 episodes: 15.24\t Episode return: 102\t Total steps: 1933\n",
      "Avg return in last 50 episodes: 16.18\t Episode return: 58\t Total steps: 1991\n",
      "Avg return in last 50 episodes: 17.02\t Episode return: 51\t Total steps: 2042\n",
      "Avg return in last 50 episodes: 18.42\t Episode return: 80\t Total steps: 2122\n",
      "Avg return in last 50 episodes: 21.32\t Episode return: 154\t Total steps: 2276\n",
      "Avg return in last 50 episodes: 22.14\t Episode return: 52\t Total steps: 2328\n",
      "Avg return in last 50 episodes: 23.52\t Episode return: 79\t Total steps: 2407\n",
      "Avg return in last 50 episodes: 25.94\t Episode return: 130\t Total steps: 2537\n"
     ]
    }
   ],
   "source": [
    "// Training the agent for a total of at least 2500 steps.\n",
    "train(2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total steps: 200\t Total reward: 200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c77176e7424e15b4476737c42206f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Deterministic() = true;\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {double(agent.Action().action)};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += env.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little more training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps.\n",
      "Avg return in last 50 episodes: 28.42\t Episode return: 134\t Total steps: 2671\n",
      "Avg return in last 50 episodes: 29.16\t Episode return: 46\t Total steps: 2717\n",
      "Avg return in last 50 episodes: 32.06\t Episode return: 153\t Total steps: 2870\n",
      "Avg return in last 50 episodes: 33.1\t Episode return: 61\t Total steps: 2931\n",
      "Avg return in last 50 episodes: 35.02\t Episode return: 104\t Total steps: 3035\n",
      "Avg return in last 50 episodes: 36.9\t Episode return: 103\t Total steps: 3138\n",
      "Avg return in last 50 episodes: 39.32\t Episode return: 130\t Total steps: 3268\n",
      "Avg return in last 50 episodes: 41.48\t Episode return: 116\t Total steps: 3384\n",
      "Avg return in last 50 episodes: 45.28\t Episode return: 200\t Total steps: 3584\n",
      "Avg return in last 50 episodes: 48.94\t Episode return: 193\t Total steps: 3777\n",
      "Avg return in last 50 episodes: 52.74\t Episode return: 200\t Total steps: 3977\n",
      "Avg return in last 50 episodes: 56.4\t Episode return: 200\t Total steps: 4177\n",
      "Avg return in last 50 episodes: 60.18\t Episode return: 200\t Total steps: 4377\n",
      "Avg return in last 50 episodes: 64.02\t Episode return: 200\t Total steps: 4577\n",
      "Avg return in last 50 episodes: 67.82\t Episode return: 200\t Total steps: 4777\n",
      "Avg return in last 50 episodes: 71.58\t Episode return: 200\t Total steps: 4977\n",
      "Avg return in last 50 episodes: 75.36\t Episode return: 200\t Total steps: 5177\n"
     ]
    }
   ],
   "source": [
    "// Training the same agent for a total of at least 5000 episodes.\n",
    "train(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final agent testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.Deterministic() = true;\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {double(agent.Action().action)};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += env.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
