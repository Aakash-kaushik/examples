{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![mlpack-lab Image](https://img.shields.io/endpoint?url=https%3A%2F%2Flab.kurg.org%2Fstatus%2Fstatus.json)](https://lab.mlpack.org)\n",
    "\n",
    "You can easily run this notebook at https://lab.mlpack.org/\n",
    "\n",
    "This notebook shows how to get started with training reinforcement learning agents, particularly DQN agents, using mlpack. Here, we train a [Simple DQN](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) agent to get high scores for the [CartPole](https://gym.openai.com/envs/CartPole-v0) environment. \n",
    "\n",
    "mlpack contains non-GUI implementations of some of OpenAI gym's environments. In this notebook, we use one such environment for training the agent, as it is fast to train on.\n",
    "\n",
    "As for testing, we make the agent run on OpenAI Gym toolkit's GUI interface provided through a distributed infrastructure (TCP API). More details can be found [here](https://github.com/zoq/gym_tcp_api).\n",
    "\n",
    "A video of the trained agent can be seen in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including necessary libraries and namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/core.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/methods/ann/ffn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_learning.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_networks/simple_dqn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/environment/cart_pole.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/training_config.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to run the agent on gym's environment (provided externally) for testing.\n",
    "#include <gym/environment.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to generate and display a video of the trained agent.\n",
    "#include \"xwidgets/ximage.hpp\"\n",
    "#include \"xwidgets/xvideo.hpp\"\n",
    "#include \"xwidgets/xaudio.hpp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::ann;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace ens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::rl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the network.\n",
    "SimpleDQN<> model(4, 128, 32, 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the policy and replay method.\n",
    "GreedyPolicy<CartPole> policy(1.0, 1000, 0.1, 0.99);\n",
    "RandomReplay<CartPole> replayMethod(32, 1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up training configurations.\n",
    "TrainingConfig config;\n",
    "config.StepSize() = 0.001;\n",
    "config.Discount() = 0.99;\n",
    "config.TargetNetworkSyncInterval() = 100;\n",
    "config.ExplorationSteps() = 100;\n",
    "config.DoubleQLearning() = false;\n",
    "config.StepLimit() = 200;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up DQN agent.\n",
    "QLearning<CartPole, decltype(model), AdamUpdate, decltype(policy)>\n",
    "  agent(std::move(config), std::move(model), std::move(policy),\n",
    "  std::move(replayMethod));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Initializing training variables.\n",
    "arma::running_stat<double> averageReturn;\n",
    "size_t episodes = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Function to train the agent on mlpack's own implementation of the CartPole environment.\n",
    "void train(const size_t threshold)\n",
    "{\n",
    "  agent.Deterministic() = false;\n",
    "  while (true)\n",
    "  {\n",
    "    double episodeReturn = agent.Episode();\n",
    "    averageReturn(episodeReturn);\n",
    "    episodes += 1;\n",
    "\n",
    "    if(episodes % 10 == 0)\n",
    "    {\n",
    "      std::cout << \"Average return: \" << averageReturn.mean()\n",
    "          << \"\\t Episode return: \" << episodeReturn \n",
    "          << \"\\t Episode number: \" << episodes << std::endl;\n",
    "    }\n",
    "\n",
    "    if (episodes > threshold)\n",
    "    {\n",
    "      agent.Deterministic() = true;\n",
    "      arma::running_stat<double> testReturn;\n",
    "      for (size_t i = 0; i < 100; ++i)\n",
    "        testReturn(agent.Episode());\n",
    "\n",
    "      std::cout << \"Average return in 100 consecutive trials : \"\n",
    "          << testReturn.mean() << std::endl;\n",
    "      break;\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let the training begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return: 15.8\t Episode return: 21\t Episode number: 10\n",
      "Average return: 17.9\t Episode return: 21\t Episode number: 20\n",
      "Average return: 17.6667\t Episode return: 14\t Episode number: 30\n",
      "Average return: 17.075\t Episode return: 9\t Episode number: 40\n",
      "Average return: 17.14\t Episode return: 13\t Episode number: 50\n",
      "Average return: 19.0833\t Episode return: 46\t Episode number: 60\n",
      "Average return: 27.1143\t Episode return: 74\t Episode number: 70\n",
      "Average return: 33.25\t Episode return: 52\t Episode number: 80\n",
      "Average return: 42.2444\t Episode return: 141\t Episode number: 90\n",
      "Average return: 55.54\t Episode return: 10\t Episode number: 100\n",
      "Average return: 55.7182\t Episode return: 16\t Episode number: 110\n",
      "Average return: 63.875\t Episode return: 200\t Episode number: 120\n",
      "Average return in 100 consecutive trials : 167.63\n"
     ]
    }
   ],
   "source": [
    "// Training the agent for a total of 120 episodes.\n",
    "train(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total steps: 186\t Total reward: 186\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2817382cf8824db8bee1fdc02d9a515e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment env(\"gym.kurg.org\", \"4040\", \"CartPole-v0\");\n",
    "env.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "env.reset();\n",
    "env.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = env.observation;\n",
    "\n",
    "  // With the given state, the agent performs an action according to its defined policy.\n",
    "  agent.Step();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {double(agent.Action())};\n",
    "\n",
    "  env.step(action);\n",
    "  totalReward += env.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (env.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "env.close();\n",
    "std::string url = env.url();\n",
    "\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little more training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return: 69.2692\t Episode return: 129\t Episode number: 130\n",
      "Average return: 71.9\t Episode return: 102\t Episode number: 140\n",
      "Average return: 74.2267\t Episode return: 200\t Episode number: 150\n",
      "Average return: 76.375\t Episode return: 14\t Episode number: 160\n",
      "Average return: 76.4529\t Episode return: 14\t Episode number: 170\n",
      "Average return: 78.8\t Episode return: 156\t Episode number: 180\n",
      "Average return: 83.8421\t Episode return: 200\t Episode number: 190\n",
      "Average return: 88.85\t Episode return: 184\t Episode number: 200\n",
      "Average return: 92.3857\t Episode return: 142\t Episode number: 210\n",
      "Average return: 96.0636\t Episode return: 200\t Episode number: 220\n",
      "Average return: 100.496\t Episode return: 200\t Episode number: 230\n",
      "Average return: 104.642\t Episode return: 200\t Episode number: 240\n",
      "Average return: 103.104\t Episode return: 10\t Episode number: 250\n",
      "Average return: 101.885\t Episode return: 66\t Episode number: 260\n",
      "Average return: 101.863\t Episode return: 126\t Episode number: 270\n",
      "Average return: 103.618\t Episode return: 186\t Episode number: 280\n",
      "Average return: 106.828\t Episode return: 200\t Episode number: 290\n",
      "Average return: 109.933\t Episode return: 200\t Episode number: 300\n",
      "Average return: 112.839\t Episode return: 200\t Episode number: 310\n",
      "Average return: 115.562\t Episode return: 200\t Episode number: 320\n",
      "Average return: 118.121\t Episode return: 200\t Episode number: 330\n",
      "Average return: 120.529\t Episode return: 200\t Episode number: 340\n",
      "Average return: 122.8\t Episode return: 200\t Episode number: 350\n",
      "Average return: 124.525\t Episode return: 200\t Episode number: 360\n",
      "Average return: 126.565\t Episode return: 200\t Episode number: 370\n",
      "Average return: 128.497\t Episode return: 200\t Episode number: 380\n",
      "Average return: 130.331\t Episode return: 200\t Episode number: 390\n",
      "Average return: 128.885\t Episode return: 9\t Episode number: 400\n",
      "Average return: 128.854\t Episode return: 200\t Episode number: 410\n",
      "Average return: 130.548\t Episode return: 200\t Episode number: 420\n",
      "Average return: 132.163\t Episode return: 200\t Episode number: 430\n",
      "Average return: 133.705\t Episode return: 200\t Episode number: 440\n",
      "Average return: 134.484\t Episode return: 200\t Episode number: 450\n",
      "Average return: 135.909\t Episode return: 200\t Episode number: 460\n",
      "Average return: 137.272\t Episode return: 200\t Episode number: 470\n",
      "Average return: 138.579\t Episode return: 200\t Episode number: 480\n",
      "Average return: 139.833\t Episode return: 200\t Episode number: 490\n",
      "Average return: 140.726\t Episode return: 200\t Episode number: 500\n",
      "Average return in 100 consecutive trials : 200\n"
     ]
    }
   ],
   "source": [
    "// Training the same agent for a total of 500 episodes.\n",
    "train(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final agent testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total steps: 200\t Total reward: 200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d1a73a1b7b48628333f8535a900856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment env(\"gym.kurg.org\", \"4040\", \"CartPole-v0\");\n",
    "env.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "env.reset();\n",
    "env.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = env.observation;\n",
    "\n",
    "  // With the given state, the agent performs an action according to its defined policy.\n",
    "  agent.Step();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {double(agent.Action())};\n",
    "\n",
    "  env.step(action);\n",
    "  totalReward += env.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (env.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "env.close();\n",
    "std::string url = env.url();\n",
    "\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
