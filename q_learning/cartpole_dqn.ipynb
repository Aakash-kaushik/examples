{"cells":[{"metadata":{},"cell_type":"markdown","source":"[![mlpack-lab Image](https://img.shields.io/endpoint?url=https%3A%2F%2Flab.kurg.org%2Fstatus%2Fstatus.json)](https://lab.mlpack.org)\n\nYou can easily run this notebook at https://lab.mlpack.org/\n\nThis notebook shows how to get started with training reinforcement learning agents, particularly DQN agents, using mlpack. Here, we train a [Simple DQN](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) agent to get high scores for the [CartPole](https://gym.openai.com/envs/CartPole-v0) environment. \n\nmlpack contains non-GUI implementations of some of OpenAI gym's environments. In this notebook, we use one such environment for training the agent, as it is fast to train on.\n\nAs for testing, we make the agent run on OpenAI Gym toolkit's GUI interface provided through a distributed infrastructure (TCP API). More details can be found [here](https://github.com/zoq/gym_tcp_api).\n\nA video of the trained agent can be seen in the end."},{"metadata":{},"cell_type":"markdown","source":"## Including necessary libraries and namespaces"},{"metadata":{"trusted":true},"cell_type":"code","source":"#include <mlpack/core.hpp>","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#include <mlpack/methods/ann/ffn.hpp>\n#include <mlpack/methods/reinforcement_learning/q_learning.hpp>\n#include <mlpack/methods/reinforcement_learning/q_networks/simple_dqn.hpp>\n#include <mlpack/methods/reinforcement_learning/environment/env_type.hpp>\n#include <mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp>\n#include <mlpack/methods/reinforcement_learning/training_config.hpp>","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"// Used to run the agent on gym's environment (provided externally) for testing.\n#include <gym/environment.hpp>","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"// Used to generate and display a video of the trained agent.\n#include \"xwidgets/ximage.hpp\"\n#include \"xwidgets/xvideo.hpp\"\n#include \"xwidgets/xaudio.hpp\"","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"using namespace mlpack;","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"using namespace mlpack::ann;","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"using namespace ens;","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"using namespace mlpack::rl;","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initializing the agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"// Set up the state and action space.\nDiscreteActionEnv::State::dimension = 4;\nDiscreteActionEnv::Action::size = 2;","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"// Set up the network.\nSimpleDQN<> model(4, 128, 32, 2);","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"// Set up the policy and replay method.\nGreedyPolicy<DiscreteActionEnv> policy(1.0, 1000, 0.1, 0.99);\nRandomReplay<DiscreteActionEnv> replayMethod(32, 2000);","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"// Set up training configurations.\nTrainingConfig config;\nconfig.StepSize() = 0.001;\nconfig.Discount() = 0.99;\nconfig.TargetNetworkSyncInterval() = 100;\nconfig.ExplorationSteps() = 100;\nconfig.DoubleQLearning() = false;\nconfig.StepLimit() = 200;","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"// Set up DQN agent.\nQLearning<DiscreteActionEnv, decltype(model), AdamUpdate, decltype(policy)>\n    agent(config, model, policy, replayMethod);","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparation for training the agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"// Set up the gym training environment.\ngym::Environment env(\"gym.kurg.org\", \"4040\", \"CartPole-v0\");\n\n// Set up the gym testing environment.\ngym::Environment envTest(\"gym.kurg.org\", \"4040\", \"CartPole-v0\");\n// Start test env monitor.\nenvTest.compression(9);\nenvTest.monitor.start(\"./dummy/\", true, true);","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"// Initializing training variables.\nstd::vector<double> returnList;\nsize_t episodes = 0;\nbool converged = true;\n// The number of episode returns to keep track of.\nsize_t consecutiveEpisodes = 50;","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"// Function to train the agent on mlpack's own implementation of the CartPole environment.\nvoid train(const size_t numSteps)\n{\n  agent.Deterministic() = false;\n  std::cout << \"Training for \" << numSteps << \" steps.\" << std::endl;\n  while (agent.TotalSteps() < numSteps)\n  {\n    double episodeReturn = 0;\n    env.reset();\n    do\n    {\n      agent.State().Data() = env.observation;\n      agent.SelectAction();\n      arma::mat action = {double(agent.Action().action)};\n\n      env.step(action);\n      DiscreteActionEnv::State nextState;\n      nextState.Data() = env.observation;\n\n      replayMethod.Store(agent.State(), agent.Action(), env.reward, nextState,\n          env.done, 0.99);\n      episodeReturn += env.reward;\n      agent.TotalSteps()++;\n      if (agent.Deterministic() || agent.TotalSteps() < config.ExplorationSteps())\n        continue;\n      agent.TrainAgent();\n    } while (!env.done);\n    returnList.push_back(episodeReturn);\n    episodes += 1;\n\n    if (returnList.size() > consecutiveEpisodes)\n      returnList.erase(returnList.begin());\n        \n    double averageReturn = std::accumulate(returnList.begin(),\n                                           returnList.end(), 0.0) /\n                           returnList.size();\n    if(episodes % 1 == 0)\n    {\n      std::cout << \"Avg return in last \" << consecutiveEpisodes\n          << \" episodes: \" << averageReturn\n          << \"\\t Episode return: \" << episodeReturn\n          << \"\\t Total steps: \" << agent.TotalSteps() << std::endl;\n    }\n  }\n}","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let the training begin"},{"metadata":{"trusted":true},"cell_type":"code","source":"// Training the agent for a total of atleast 2500 steps.\ntrain(2500)","execution_count":17,"outputs":[{"output_type":"stream","text":"Training for 2500 steps.\nAvg return in last 50 episodes: 16\t Episode return: 16\t Total steps: 16\nAvg return in last 50 episodes: 15\t Episode return: 14\t Total steps: 30\nAvg return in last 50 episodes: 16\t Episode return: 18\t Total steps: 48\nAvg return in last 50 episodes: 15\t Episode return: 12\t Total steps: 60\nAvg return in last 50 episodes: 15.2\t Episode return: 16\t Total steps: 76\nAvg return in last 50 episodes: 15.5\t Episode return: 17\t Total steps: 93\nAvg return in last 50 episodes: 17.7143\t Episode return: 31\t Total steps: 124\nAvg return in last 50 episodes: 17.625\t Episode return: 17\t Total steps: 141\nAvg return in last 50 episodes: 17\t Episode return: 12\t Total steps: 153\nAvg return in last 50 episodes: 16.5\t Episode return: 12\t Total steps: 165\nAvg return in last 50 episodes: 15.9091\t Episode return: 10\t Total steps: 175\nAvg return in last 50 episodes: 15.8333\t Episode return: 15\t Total steps: 190\nAvg return in last 50 episodes: 16.3846\t Episode return: 23\t Total steps: 213\nAvg return in last 50 episodes: 17.6429\t Episode return: 34\t Total steps: 247\nAvg return in last 50 episodes: 19.4667\t Episode return: 45\t Total steps: 292\nAvg return in last 50 episodes: 19.0625\t Episode return: 13\t Total steps: 305\nAvg return in last 50 episodes: 18.7647\t Episode return: 14\t Total steps: 319\nAvg return in last 50 episodes: 18.5556\t Episode return: 15\t Total steps: 334\nAvg return in last 50 episodes: 18.5789\t Episode return: 19\t Total steps: 353\nAvg return in last 50 episodes: 18.6\t Episode return: 19\t Total steps: 372\nAvg return in last 50 episodes: 18.5238\t Episode return: 17\t Total steps: 389\nAvg return in last 50 episodes: 18.3182\t Episode return: 14\t Total steps: 403\nAvg return in last 50 episodes: 18.0435\t Episode return: 12\t Total steps: 415\nAvg return in last 50 episodes: 17.9583\t Episode return: 16\t Total steps: 431\nAvg return in last 50 episodes: 18.04\t Episode return: 20\t Total steps: 451\nAvg return in last 50 episodes: 18.1923\t Episode return: 22\t Total steps: 473\nAvg return in last 50 episodes: 18.5185\t Episode return: 27\t Total steps: 500\nAvg return in last 50 episodes: 20.1429\t Episode return: 64\t Total steps: 564\nAvg return in last 50 episodes: 19.8276\t Episode return: 11\t Total steps: 575\nAvg return in last 50 episodes: 19.4667\t Episode return: 9\t Total steps: 584\nAvg return in last 50 episodes: 19.8387\t Episode return: 31\t Total steps: 615\nAvg return in last 50 episodes: 19.5625\t Episode return: 11\t Total steps: 626\nAvg return in last 50 episodes: 19.3333\t Episode return: 12\t Total steps: 638\nAvg return in last 50 episodes: 19.2059\t Episode return: 15\t Total steps: 653\nAvg return in last 50 episodes: 19.2286\t Episode return: 20\t Total steps: 673\nAvg return in last 50 episodes: 19.0556\t Episode return: 13\t Total steps: 686\nAvg return in last 50 episodes: 19.1622\t Episode return: 23\t Total steps: 709\nAvg return in last 50 episodes: 19.2105\t Episode return: 21\t Total steps: 730\nAvg return in last 50 episodes: 19.1538\t Episode return: 17\t Total steps: 747\nAvg return in last 50 episodes: 18.95\t Episode return: 11\t Total steps: 758\nAvg return in last 50 episodes: 19.2195\t Episode return: 30\t Total steps: 788\nAvg return in last 50 episodes: 19\t Episode return: 10\t Total steps: 798\nAvg return in last 50 episodes: 18.814\t Episode return: 11\t Total steps: 809\nAvg return in last 50 episodes: 18.5682\t Episode return: 8\t Total steps: 817\nAvg return in last 50 episodes: 18.6444\t Episode return: 22\t Total steps: 839\nAvg return in last 50 episodes: 18.5217\t Episode return: 13\t Total steps: 852\nAvg return in last 50 episodes: 18.383\t Episode return: 12\t Total steps: 864\nAvg return in last 50 episodes: 18.2083\t Episode return: 10\t Total steps: 874\nAvg return in last 50 episodes: 18.1224\t Episode return: 14\t Total steps: 888\nAvg return in last 50 episodes: 18.22\t Episode return: 23\t Total steps: 911\nAvg return in last 50 episodes: 18.2\t Episode return: 15\t Total steps: 926\nAvg return in last 50 episodes: 18.32\t Episode return: 20\t Total steps: 946\nAvg return in last 50 episodes: 18.2\t Episode return: 12\t Total steps: 958\nAvg return in last 50 episodes: 18.34\t Episode return: 19\t Total steps: 977\nAvg return in last 50 episodes: 18.32\t Episode return: 15\t Total steps: 992\nAvg return in last 50 episodes: 18.32\t Episode return: 17\t Total steps: 1009\nAvg return in last 50 episodes: 17.9\t Episode return: 10\t Total steps: 1019\nAvg return in last 50 episodes: 17.8\t Episode return: 12\t Total steps: 1031\nAvg return in last 50 episodes: 17.72\t Episode return: 8\t Total steps: 1039\nAvg return in last 50 episodes: 17.9\t Episode return: 21\t Total steps: 1060\nAvg return in last 50 episodes: 18.04\t Episode return: 17\t Total steps: 1077\nAvg return in last 50 episodes: 17.96\t Episode return: 11\t Total steps: 1088\nAvg return in last 50 episodes: 17.68\t Episode return: 9\t Total steps: 1097\nAvg return in last 50 episodes: 17.24\t Episode return: 12\t Total steps: 1109\nAvg return in last 50 episodes: 16.56\t Episode return: 11\t Total steps: 1120\nAvg return in last 50 episodes: 16.5\t Episode return: 10\t Total steps: 1130\nAvg return in last 50 episodes: 16.48\t Episode return: 13\t Total steps: 1143\nAvg return in last 50 episodes: 16.36\t Episode return: 9\t Total steps: 1152\nAvg return in last 50 episodes: 16.18\t Episode return: 10\t Total steps: 1162\nAvg return in last 50 episodes: 15.98\t Episode return: 9\t Total steps: 1171\nAvg return in last 50 episodes: 15.86\t Episode return: 11\t Total steps: 1182\nAvg return in last 50 episodes: 15.76\t Episode return: 9\t Total steps: 1191\nAvg return in last 50 episodes: 15.72\t Episode return: 10\t Total steps: 1201\nAvg return in last 50 episodes: 15.58\t Episode return: 9\t Total steps: 1210\nAvg return in last 50 episodes: 15.4\t Episode return: 11\t Total steps: 1221\nAvg return in last 50 episodes: 15.16\t Episode return: 10\t Total steps: 1231\nAvg return in last 50 episodes: 14.8\t Episode return: 9\t Total steps: 1240\nAvg return in last 50 episodes: 13.72\t Episode return: 10\t Total steps: 1250\nAvg return in last 50 episodes: 13.68\t Episode return: 9\t Total steps: 1259\nAvg return in last 50 episodes: 13.66\t Episode return: 8\t Total steps: 1267\nAvg return in last 50 episodes: 13.22\t Episode return: 9\t Total steps: 1276\nAvg return in last 50 episodes: 13.16\t Episode return: 8\t Total steps: 1284\nAvg return in last 50 episodes: 13.1\t Episode return: 9\t Total steps: 1293\nAvg return in last 50 episodes: 12.98\t Episode return: 9\t Total steps: 1302\nAvg return in last 50 episodes: 12.74\t Episode return: 8\t Total steps: 1310\nAvg return in last 50 episodes: 12.68\t Episode return: 10\t Total steps: 1320\nAvg return in last 50 episodes: 12.42\t Episode return: 10\t Total steps: 1330\nAvg return in last 50 episodes: 12.2\t Episode return: 10\t Total steps: 1340\nAvg return in last 50 episodes: 12.2\t Episode return: 17\t Total steps: 1357\nAvg return in last 50 episodes: 12.2\t Episode return: 11\t Total steps: 1368\nAvg return in last 50 episodes: 11.76\t Episode return: 8\t Total steps: 1376\nAvg return in last 50 episodes: 11.76\t Episode return: 10\t Total steps: 1386\nAvg return in last 50 episodes: 11.78\t Episode return: 12\t Total steps: 1398\nAvg return in last 50 episodes: 11.84\t Episode return: 11\t Total steps: 1409\nAvg return in last 50 episodes: 11.6\t Episode return: 10\t Total steps: 1419\nAvg return in last 50 episodes: 11.52\t Episode return: 9\t Total steps: 1428\nAvg return in last 50 episodes: 11.48\t Episode return: 10\t Total steps: 1438\nAvg return in last 50 episodes: 11.46\t Episode return: 9\t Total steps: 1447\nAvg return in last 50 episodes: 11.38\t Episode return: 10\t Total steps: 1457\nAvg return in last 50 episodes: 11.3\t Episode return: 19\t Total steps: 1476\nAvg return in last 50 episodes: 11.18\t Episode return: 9\t Total steps: 1485\nAvg return in last 50 episodes: 11.16\t Episode return: 19\t Total steps: 1504\nAvg return in last 50 episodes: 11.12\t Episode return: 10\t Total steps: 1514\nAvg return in last 50 episodes: 10.94\t Episode return: 10\t Total steps: 1524\nAvg return in last 50 episodes: 10.9\t Episode return: 13\t Total steps: 1537\nAvg return in last 50 episodes: 10.76\t Episode return: 10\t Total steps: 1547\nAvg return in last 50 episodes: 10.78\t Episode return: 11\t Total steps: 1558\n","name":"stdout"},{"output_type":"stream","text":"Avg return in last 50 episodes: 11.1\t Episode return: 28\t Total steps: 1586\nAvg return in last 50 episodes: 11.12\t Episode return: 9\t Total steps: 1595\nAvg return in last 50 episodes: 10.9\t Episode return: 10\t Total steps: 1605\nAvg return in last 50 episodes: 10.84\t Episode return: 14\t Total steps: 1619\nAvg return in last 50 episodes: 10.78\t Episode return: 8\t Total steps: 1627\nAvg return in last 50 episodes: 10.78\t Episode return: 9\t Total steps: 1636\nAvg return in last 50 episodes: 10.72\t Episode return: 9\t Total steps: 1645\nAvg return in last 50 episodes: 11.46\t Episode return: 48\t Total steps: 1693\nAvg return in last 50 episodes: 11.46\t Episode return: 10\t Total steps: 1703\nAvg return in last 50 episodes: 11.46\t Episode return: 13\t Total steps: 1716\nAvg return in last 50 episodes: 12.58\t Episode return: 65\t Total steps: 1781\nAvg return in last 50 episodes: 13.38\t Episode return: 50\t Total steps: 1831\nAvg return in last 50 episodes: 15.24\t Episode return: 102\t Total steps: 1933\nAvg return in last 50 episodes: 16.18\t Episode return: 58\t Total steps: 1991\nAvg return in last 50 episodes: 17.02\t Episode return: 51\t Total steps: 2042\nAvg return in last 50 episodes: 18.42\t Episode return: 80\t Total steps: 2122\nAvg return in last 50 episodes: 21.32\t Episode return: 154\t Total steps: 2276\nAvg return in last 50 episodes: 22.14\t Episode return: 52\t Total steps: 2328\nAvg return in last 50 episodes: 23.52\t Episode return: 79\t Total steps: 2407\nAvg return in last 50 episodes: 25.94\t Episode return: 130\t Total steps: 2537\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Testing the trained agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent.Deterministic() = true;\n\n// Resets the environment.\nenvTest.reset();\nenvTest.render();\n\ndouble totalReward = 0;\nsize_t totalSteps = 0;\n\n// Testing the agent on gym's environment.\nwhile (1)\n{\n  // State from the environment is passed to the agent's internal representation.\n  agent.State().Data() = envTest.observation;\n\n  // With the given state, the agent selects an action according to its defined policy.\n  agent.SelectAction();\n\n  // Action to take, decided by the policy.\n  arma::mat action = {double(agent.Action().action)};\n\n  envTest.step(action);\n  totalReward += env.reward;\n  totalSteps += 1;\n\n  if (envTest.done)\n  {\n    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n        << totalReward << std::endl;\n    break;\n  }\n\n  // Uncomment the following lines to see the reward and action in each step.\n  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n  //   << totalReward << \"\\t Action taken: \" << action;\n}\n\nenvTest.close();\nstd::string url = envTest.url();\n\nauto video = xw::video_from_url(url).finalize();\nvideo","execution_count":18,"outputs":[{"output_type":"stream","text":" Total steps: 200\t Total reward: 200\n","name":"stdout"},{"output_type":"execute_result","execution_count":18,"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"54c77176e7424e15b4476737c42206f5","version_major":2,"version_minor":0},"text/plain":"A Jupyter widget"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## A little more training..."},{"metadata":{"trusted":true},"cell_type":"code","source":"// Training the same agent for a total of atleast 5000 episodes.\ntrain(5000)","execution_count":19,"outputs":[{"output_type":"stream","text":"Training for 5000 steps.\nAvg return in last 50 episodes: 28.42\t Episode return: 134\t Total steps: 2671\nAvg return in last 50 episodes: 29.16\t Episode return: 46\t Total steps: 2717\nAvg return in last 50 episodes: 32.06\t Episode return: 153\t Total steps: 2870\nAvg return in last 50 episodes: 33.1\t Episode return: 61\t Total steps: 2931\nAvg return in last 50 episodes: 35.02\t Episode return: 104\t Total steps: 3035\nAvg return in last 50 episodes: 36.9\t Episode return: 103\t Total steps: 3138\nAvg return in last 50 episodes: 39.32\t Episode return: 130\t Total steps: 3268\nAvg return in last 50 episodes: 41.48\t Episode return: 116\t Total steps: 3384\nAvg return in last 50 episodes: 45.28\t Episode return: 200\t Total steps: 3584\nAvg return in last 50 episodes: 48.94\t Episode return: 193\t Total steps: 3777\nAvg return in last 50 episodes: 52.74\t Episode return: 200\t Total steps: 3977\nAvg return in last 50 episodes: 56.4\t Episode return: 200\t Total steps: 4177\nAvg return in last 50 episodes: 60.18\t Episode return: 200\t Total steps: 4377\nAvg return in last 50 episodes: 64.02\t Episode return: 200\t Total steps: 4577\nAvg return in last 50 episodes: 67.82\t Episode return: 200\t Total steps: 4777\nAvg return in last 50 episodes: 71.58\t Episode return: 200\t Total steps: 4977\nAvg return in last 50 episodes: 75.36\t Episode return: 200\t Total steps: 5177\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Final agent testing!"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent.Deterministic() = true;\n\n// Resets the environment.\nenvTest.reset();\nenvTest.render();\n\ndouble totalReward = 0;\nsize_t totalSteps = 0;\n\n// Testing the agent on gym's environment.\nwhile (1)\n{\n  // State from the environment is passed to the agent's internal representation.\n  agent.State().Data() = envTest.observation;\n\n  // With the given state, the agent selects an action according to its defined policy.\n  agent.SelectAction();\n\n  // Action to take, decided by the policy.\n  arma::mat action = {double(agent.Action().action)};\n\n  envTest.step(action);\n  totalReward += env.reward;\n  totalSteps += 1;\n\n  if (envTest.done)\n  {\n    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n        << totalReward << std::endl;\n    break;\n  }\n\n  // Uncomment the following lines to see the reward and action in each step.\n  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n  //   << totalReward << \"\\t Action taken: \" << action;\n}\n\nenvTest.close();\nstd::string url = envTest.url();\n\nauto video = xw::video_from_url(url).finalize();\nvideo","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"xcpp14","display_name":"C++14","language":"C++14"},"language_info":{"codemirror_mode":"text/x-c++src","file_extension":".cpp","mimetype":"text/x-c++src","name":"c++","version":"14"}},"nbformat":4,"nbformat_minor":2}