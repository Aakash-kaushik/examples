{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![mlpack-lab Image](https://img.shields.io/endpoint?url=https%3A%2F%2Flab.kurg.org%2Fstatus%2Fstatus.json)](https://lab.mlpack.org)\n",
    "\n",
    "You can easily run this notebook at https://lab.mlpack.org/\n",
    "\n",
    "This notebook shows how to get started with training reinforcement learning agents, particularly DQN agents, using mlpack. Here, we train a [Simple DQN](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) agent to get high scores for the [CartPole](https://gym.openai.com/envs/CartPole-v0) environment. \n",
    "\n",
    "Mlpack contains non-gui implementations of some of OpenAI gym's environments. In this notebook, we use one such environment for training the agent, as it is fast to train on.\n",
    "\n",
    "As for testing, we make the agent run on OpenAI Gym toolkit's GUI interface provided through a distributed infrastructure (TCP API). A video of the trained agent can be seen in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including necessary libraries and namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/core.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/methods/ann/ffn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_learning.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_networks/simple_dqn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/environment/cart_pole.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/training_config.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to run the agent on gym's environment (provided externally) for testing\n",
    "#include <gym/environment.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to generate and display a video of the trained agent.\n",
    "#include \"xwidgets/ximage.hpp\"\n",
    "#include \"xwidgets/xvideo.hpp\"\n",
    "#include \"xwidgets/xaudio.hpp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::ann;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace ens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::rl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the network.\n",
    "SimpleDQN<> model(4, 256, 128, 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the policy and replay method.\n",
    "GreedyPolicy<CartPole> policy(1.0, 1000, 0.1, 0.99);\n",
    "RandomReplay<CartPole> replayMethod(20, 10000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up training configurations.\n",
    "TrainingConfig config;\n",
    "config.StepSize() = 0.01;\n",
    "config.Discount() = 0.9;\n",
    "config.TargetNetworkSyncInterval() = 100;\n",
    "config.ExplorationSteps() = 100;\n",
    "config.DoubleQLearning() = false;\n",
    "config.StepLimit() = 200;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up DQN agent.\n",
    "QLearning<CartPole, decltype(model), AdamUpdate, decltype(policy)>\n",
    "  agent(std::move(config), std::move(model), std::move(policy),\n",
    "  std::move(replayMethod));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return: 21.5\t Episode return: 28\t Episode number: 10\n",
      "Average return: 19.85\t Episode return: 13\t Episode number: 20\n",
      "Average return: 21.8667\t Episode return: 39\t Episode number: 30\n",
      "Average return: 43.175\t Episode return: 146\t Episode number: 40\n",
      "Average return in deterministic test: 145.1\n"
     ]
    }
   ],
   "source": [
    "// Training the agent on mlpack's own implementation of the CartPole environment:\n",
    "arma::running_stat<double> averageReturn;\n",
    "size_t episodes = 0;\n",
    "\n",
    "while (true)\n",
    "{\n",
    "    double episodeReturn = agent.Episode();\n",
    "    averageReturn(episodeReturn);\n",
    "    episodes += 1;\n",
    "    \n",
    "    if(episodes % 10 == 0)\n",
    "        std::cout << \"Average return: \" << averageReturn.mean()\n",
    "            << \"\\t Episode return: \" << episodeReturn \n",
    "            << \"\\t Episode number: \" << episodes << std::endl;\n",
    "    \n",
    "    if (episodes > 1000)\n",
    "    {\n",
    "      std::cout << \"Cart Pole with DQN failed.\" << std::endl;\n",
    "      break;\n",
    "    }\n",
    "    \n",
    "    if (averageReturn.mean() > 50)\n",
    "    {\n",
    "      agent.Deterministic() = true;\n",
    "      arma::running_stat<double> testReturn;\n",
    "      for (size_t i = 0; i < 10; ++i)\n",
    "        testReturn(agent.Episode());\n",
    "\n",
    "      std::cout << \"Average return in deterministic test: \"\n",
    "          << testReturn.mean() << std::endl;\n",
    "      break;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym::Environment env(\"gym.kurg.org\", \"4040\", \"CartPole-v0\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.monitor.start(\"./dummy/\", true, true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@0x7fb1a0d8d530"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:    1.0000\n",
      " Current step: 1\t current reward: 1\n",
      "action:    1.0000\n",
      " Current step: 2\t current reward: 2\n",
      "action:    1.0000\n",
      " Current step: 3\t current reward: 3\n",
      "action:         0\n",
      " Current step: 4\t current reward: 4\n",
      "action:         0\n",
      " Current step: 5\t current reward: 5\n",
      "action:    1.0000\n",
      " Current step: 6\t current reward: 6\n",
      "action:         0\n",
      " Current step: 7\t current reward: 7\n",
      "action:         0\n",
      " Current step: 8\t current reward: 8\n",
      "action:    1.0000\n",
      " Current step: 9\t current reward: 9\n",
      "action:         0\n",
      " Current step: 10\t current reward: 10\n",
      "action:    1.0000\n",
      " Current step: 11\t current reward: 11\n",
      "action:         0\n",
      " Current step: 12\t current reward: 12\n",
      "action:    1.0000\n",
      " Current step: 13\t current reward: 13\n",
      "action:         0\n",
      " Current step: 14\t current reward: 14\n",
      "action:    1.0000\n",
      " Current step: 15\t current reward: 15\n",
      "action:         0\n",
      " Current step: 16\t current reward: 16\n",
      "action:    1.0000\n",
      " Current step: 17\t current reward: 17\n",
      "action:         0\n",
      " Current step: 18\t current reward: 18\n",
      "action:    1.0000\n",
      " Current step: 19\t current reward: 19\n",
      "action:         0\n",
      " Current step: 20\t current reward: 20\n",
      "action:    1.0000\n",
      " Current step: 21\t current reward: 21\n",
      "action:         0\n",
      " Current step: 22\t current reward: 22\n",
      "action:    1.0000\n",
      " Current step: 23\t current reward: 23\n",
      "action:         0\n",
      " Current step: 24\t current reward: 24\n",
      "action:         0\n",
      " Current step: 25\t current reward: 25\n",
      "action:    1.0000\n",
      " Current step: 26\t current reward: 26\n",
      "action:         0\n",
      " Current step: 27\t current reward: 27\n",
      "action:    1.0000\n",
      " Current step: 28\t current reward: 28\n",
      "action:    1.0000\n",
      " Current step: 29\t current reward: 29\n",
      "action:         0\n",
      " Current step: 30\t current reward: 30\n",
      "action:         0\n",
      " Current step: 31\t current reward: 31\n",
      "action:    1.0000\n",
      " Current step: 32\t current reward: 32\n",
      "action:         0\n",
      " Current step: 33\t current reward: 33\n",
      "action:    1.0000\n",
      " Current step: 34\t current reward: 34\n",
      "action:         0\n",
      " Current step: 35\t current reward: 35\n",
      "action:    1.0000\n",
      " Current step: 36\t current reward: 36\n",
      "action:    1.0000\n",
      " Current step: 37\t current reward: 37\n",
      "action:         0\n",
      " Current step: 38\t current reward: 38\n",
      "action:         0\n",
      " Current step: 39\t current reward: 39\n",
      "action:    1.0000\n",
      " Current step: 40\t current reward: 40\n",
      "action:         0\n",
      " Current step: 41\t current reward: 41\n",
      "action:    1.0000\n",
      " Current step: 42\t current reward: 42\n",
      "action:         0\n",
      " Current step: 43\t current reward: 43\n",
      "action:    1.0000\n",
      " Current step: 44\t current reward: 44\n",
      "action:    1.0000\n",
      " Current step: 45\t current reward: 45\n",
      "action:         0\n",
      " Current step: 46\t current reward: 46\n",
      "action:         0\n",
      " Current step: 47\t current reward: 47\n",
      "action:    1.0000\n",
      " Current step: 48\t current reward: 48\n",
      "action:         0\n",
      " Current step: 49\t current reward: 49\n",
      "action:    1.0000\n",
      " Current step: 50\t current reward: 50\n",
      "action:         0\n",
      " Current step: 51\t current reward: 51\n",
      "action:    1.0000\n",
      " Current step: 52\t current reward: 52\n",
      "action:         0\n",
      " Current step: 53\t current reward: 53\n",
      "action:    1.0000\n",
      " Current step: 54\t current reward: 54\n",
      "action:         0\n",
      " Current step: 55\t current reward: 55\n",
      "action:    1.0000\n",
      " Current step: 56\t current reward: 56\n",
      "action:         0\n",
      " Current step: 57\t current reward: 57\n",
      "action:    1.0000\n",
      " Current step: 58\t current reward: 58\n",
      "action:         0\n",
      " Current step: 59\t current reward: 59\n",
      "action:    1.0000\n",
      " Current step: 60\t current reward: 60\n",
      "action:         0\n",
      " Current step: 61\t current reward: 61\n",
      "action:    1.0000\n",
      " Current step: 62\t current reward: 62\n",
      "action:         0\n",
      " Current step: 63\t current reward: 63\n",
      "action:    1.0000\n",
      " Current step: 64\t current reward: 64\n",
      "action:         0\n",
      " Current step: 65\t current reward: 65\n",
      "action:    1.0000\n",
      " Current step: 66\t current reward: 66\n",
      "action:         0\n",
      " Current step: 67\t current reward: 67\n",
      "action:    1.0000\n",
      " Current step: 68\t current reward: 68\n",
      "action:         0\n",
      " Current step: 69\t current reward: 69\n",
      "action:    1.0000\n",
      " Current step: 70\t current reward: 70\n",
      "action:         0\n",
      " Current step: 71\t current reward: 71\n",
      "action:    1.0000\n",
      " Current step: 72\t current reward: 72\n",
      "action:         0\n",
      " Current step: 73\t current reward: 73\n",
      "action:    1.0000\n",
      " Current step: 74\t current reward: 74\n",
      "action:         0\n",
      " Current step: 75\t current reward: 75\n",
      "action:    1.0000\n",
      " Current step: 76\t current reward: 76\n",
      "action:         0\n",
      " Current step: 77\t current reward: 77\n",
      "action:    1.0000\n",
      " Current step: 78\t current reward: 78\n",
      "action:         0\n",
      " Current step: 79\t current reward: 79\n",
      "action:    1.0000\n",
      " Current step: 80\t current reward: 80\n",
      "action:         0\n",
      " Current step: 81\t current reward: 81\n",
      "action:    1.0000\n",
      " Current step: 82\t current reward: 82\n",
      "action:         0\n",
      " Current step: 83\t current reward: 83\n",
      "action:    1.0000\n",
      " Current step: 84\t current reward: 84\n",
      "action:         0\n",
      " Current step: 85\t current reward: 85\n",
      "action:    1.0000\n",
      " Current step: 86\t current reward: 86\n",
      "action:         0\n",
      " Current step: 87\t current reward: 87\n",
      "action:    1.0000\n",
      " Current step: 88\t current reward: 88\n",
      "action:         0\n",
      " Current step: 89\t current reward: 89\n",
      "action:    1.0000\n",
      " Current step: 90\t current reward: 90\n",
      "action:         0\n",
      " Current step: 91\t current reward: 91\n",
      "action:    1.0000\n",
      " Current step: 92\t current reward: 92\n",
      "action:         0\n",
      " Current step: 93\t current reward: 93\n",
      "action:    1.0000\n",
      " Current step: 94\t current reward: 94\n",
      "action:         0\n",
      " Current step: 95\t current reward: 95\n",
      "action:    1.0000\n",
      " Current step: 96\t current reward: 96\n",
      "action:         0\n",
      " Current step: 97\t current reward: 97\n",
      "action:    1.0000\n",
      " Current step: 98\t current reward: 98\n",
      "action:         0\n",
      " Current step: 99\t current reward: 99\n",
      "action:         0\n",
      " Current step: 100\t current reward: 100\n",
      "action:    1.0000\n",
      " Current step: 101\t current reward: 101\n",
      "action:         0\n",
      " Current step: 102\t current reward: 102\n",
      "action:    1.0000\n",
      " Current step: 103\t current reward: 103\n",
      "action:         0\n",
      " Current step: 104\t current reward: 104\n",
      "action:         0\n",
      " Current step: 105\t current reward: 105\n",
      "action:    1.0000\n",
      " Current step: 106\t current reward: 106\n",
      "action:         0\n",
      " Current step: 107\t current reward: 107\n",
      "action:    1.0000\n",
      " Current step: 108\t current reward: 108\n",
      "action:         0\n",
      " Current step: 109\t current reward: 109\n",
      "action:    1.0000\n",
      " Current step: 110\t current reward: 110\n",
      "action:         0\n",
      " Current step: 111\t current reward: 111\n",
      "action:         0\n",
      " Current step: 112\t current reward: 112\n",
      "action:    1.0000\n",
      " Current step: 113\t current reward: 113\n",
      "action:         0\n",
      " Current step: 114\t current reward: 114\n",
      "action:    1.0000\n",
      " Current step: 115\t current reward: 115\n",
      "action:         0\n",
      " Current step: 116\t current reward: 116\n",
      "action:    1.0000\n",
      " Current step: 117\t current reward: 117\n",
      "action:         0\n",
      " Current step: 118\t current reward: 118\n",
      "action:         0\n",
      " Current step: 119\t current reward: 119\n",
      "action:    1.0000\n",
      " Current step: 120\t current reward: 120\n",
      "action:         0\n",
      " Current step: 121\t current reward: 121\n",
      "action:    1.0000\n",
      " Current step: 122\t current reward: 122\n",
      "action:         0\n",
      " Current step: 123\t current reward: 123\n",
      "action:    1.0000\n",
      " Current step: 124\t current reward: 124\n",
      "action:         0\n",
      " Current step: 125\t current reward: 125\n",
      "action:    1.0000\n",
      " Current step: 126\t current reward: 126\n",
      "action:         0\n",
      " Current step: 127\t current reward: 127\n",
      "action:    1.0000\n",
      " Current step: 128\t current reward: 128\n",
      "action:         0\n",
      " Current step: 129\t current reward: 129\n",
      "action:         0\n",
      " Current step: 130\t current reward: 130\n",
      "action:    1.0000\n",
      " Current step: 131\t current reward: 131\n",
      "action:         0\n",
      " Current step: 132\t current reward: 132\n",
      "action:    1.0000\n",
      " Current step: 133\t current reward: 133\n",
      "action:         0\n",
      " Current step: 134\t current reward: 134\n",
      "action:    1.0000\n",
      " Current step: 135\t current reward: 135\n",
      "action:         0\n",
      " Current step: 136\t current reward: 136\n",
      "action:         0\n",
      " Current step: 137\t current reward: 137\n",
      "action:    1.0000\n",
      " Current step: 138\t current reward: 138\n",
      "action:         0\n",
      " Current step: 139\t current reward: 139\n",
      "action:    1.0000\n",
      " Current step: 140\t current reward: 140\n",
      "action:         0\n",
      " Current step: 141\t current reward: 141\n",
      "action:    1.0000\n",
      " Current step: 142\t current reward: 142\n",
      "action:         0\n",
      " Current step: 143\t current reward: 143\n",
      "action:    1.0000\n",
      " Current step: 144\t current reward: 144\n",
      "action:         0\n",
      " Current step: 145\t current reward: 145\n",
      "action:    1.0000\n",
      " Current step: 146\t current reward: 146\n",
      "action:         0\n",
      " Current step: 147\t current reward: 147\n",
      "action:         0\n",
      " Current step: 148\t current reward: 148\n",
      "action:    1.0000\n",
      " Current step: 149\t current reward: 149\n",
      "action:         0\n",
      " Current step: 150\t current reward: 150\n",
      "action:    1.0000\n",
      " Current step: 151\t current reward: 151\n",
      "action:         0\n",
      " Current step: 152\t current reward: 152\n",
      "action:         0\n",
      " Current step: 153\t current reward: 153\n",
      "action:         0\n",
      " Current step: 154\t current reward: 154\n",
      "action:         0\n",
      " Current step: 155\t current reward: 155\n",
      "action:         0\n",
      " Current step: 156\t current reward: 156\n",
      "action:         0\n",
      " Current step: 157\t current reward: 157\n",
      "action:         0\n",
      " Current step: 158\t current reward: 158\n",
      "action:    1.0000\n",
      " Current step: 159\t current reward: 159\n",
      "action:    1.0000\n",
      " Current step: 160\t current reward: 160\n",
      "action:    1.0000\n",
      " Current step: 161\t current reward: 161\n",
      "action:    1.0000\n",
      " Current step: 162\t current reward: 162\n",
      "action:    1.0000\n",
      " Current step: 163\t current reward: 163\n",
      "action:    1.0000\n",
      " Current step: 164\t current reward: 164\n",
      "action:    1.0000\n",
      " Current step: 165\t current reward: 165\n",
      "action:    1.0000\n",
      " Current step: 166\t current reward: 166\n",
      "action:    1.0000\n",
      " Current step: 167\t current reward: 167\n",
      "action:    1.0000\n",
      " Current step: 168\t current reward: 168\n",
      "action:    1.0000\n",
      " Current step: 169\t current reward: 169\n",
      "action:         0\n",
      " Current step: 170\t current reward: 170\n",
      "action:         0\n",
      " Current step: 171\t current reward: 171\n",
      "action:    1.0000\n",
      " Current step: 172\t current reward: 172\n",
      "action:         0\n",
      " Current step: 173\t current reward: 173\n",
      "action:         0\n",
      " Current step: 174\t current reward: 174\n",
      "action:         0\n",
      " Current step: 175\t current reward: 175\n",
      "action:         0\n",
      " Current step: 176\t current reward: 176\n",
      "action:         0\n",
      " Current step: 177\t current reward: 177\n",
      "action:    1.0000\n",
      " Current step: 178\t current reward: 178\n",
      "action:         0\n",
      " Current step: 179\t current reward: 179\n",
      "action:         0\n",
      " Current step: 180\t current reward: 180\n",
      "action:         0\n",
      " Current step: 181\t current reward: 181\n",
      "action:         0\n",
      " Current step: 182\t current reward: 182\n",
      "action:         0\n",
      " Current step: 183\t current reward: 183\n",
      "action:         0\n",
      " Current step: 184\t current reward: 184\n",
      "action:         0\n",
      " Current step: 185\t current reward: 185\n",
      "action:    1.0000\n",
      " Current step: 186\t current reward: 186\n",
      "action:    1.0000\n",
      " Current step: 187\t current reward: 187\n",
      "action:    1.0000\n",
      " Current step: 188\t current reward: 188\n",
      "action:    1.0000\n",
      " Current step: 189\t current reward: 189\n",
      "action:    1.0000\n",
      " Current step: 190\t current reward: 190\n",
      "action:    1.0000\n",
      " Current step: 191\t current reward: 191\n",
      "action:    1.0000\n",
      " Current step: 192\t current reward: 192\n",
      "action:    1.0000\n",
      " Current step: 193\t current reward: 193\n",
      "action:    1.0000\n",
      " Current step: 194\t current reward: 194\n",
      "action:         0\n",
      " Current step: 195\t current reward: 195\n",
      "action:         0\n",
      " Current step: 196\t current reward: 196\n",
      "action:         0\n",
      " Current step: 197\t current reward: 197\n",
      "action:         0\n",
      " Current step: 198\t current reward: 198\n",
      "action:    1.0000\n",
      " Current step: 199\t current reward: 199\n",
      "action:         0\n"
     ]
    }
   ],
   "source": [
    "// Testing the agent on gym's environment\n",
    "while (1)\n",
    "  {\n",
    "    // State from the environment is passed to the agent's internal representation\n",
    "    agent.State().Data() = env.observation;\n",
    "    \n",
    "    // with the given state, the agent performs an action according to its defined policy\n",
    "    agent.Step();\n",
    "    \n",
    "    // Action to take, decided by the policy\n",
    "    arma::mat action = {double(agent.Action())};\n",
    "\n",
    "    std::cout << \"action: \" << action;\n",
    "\n",
    "    env.step(action);\n",
    "    totalReward += env.reward;\n",
    "    totalSteps += 1;\n",
    "\n",
    "    if (env.done)\n",
    "      break;\n",
    "\n",
    "    std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "              << totalReward << std::endl;\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close();\n",
    "std::string url = env.url();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4cdbbc11204498acb9f36bd583e627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
