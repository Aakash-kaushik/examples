{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://lab.mlpack.org/v2/gh/mlpack/examples/master?urlpath=lab%2Ftree%2Fq_learning%2Fcartpole_dqn.ipynb)\n",
    "\n",
    "You can easily run this notebook at https://lab.mlpack.org/\n",
    "\n",
    "This notebook shows how to get started with training reinforcement learning agents, particularly DQN agents, using mlpack. Here, we train a [Simple DQN](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) agent to get high scores for the [LunarLander-v2](https://gym.openai.com/envs/LunarLander-v2/) environment. \n",
    "\n",
    "We make the agent train and test on OpenAI Gym toolkit's GUI interface provided through a distributed infrastructure (TCP API). More details can be found [here](https://github.com/zoq/gym_tcp_api).\n",
    "\n",
    "A video of the trained agent can be seen in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including necessary libraries and namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/core.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/methods/ann/ffn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_learning.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_networks/simple_dqn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/environment/env_type.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/training_config.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to run the agent on gym's environment (provided externally) for testing.\n",
    "#include <gym/environment.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to generate and display a video of the trained agent.\n",
    "#include \"xwidgets/ximage.hpp\"\n",
    "#include \"xwidgets/xvideo.hpp\"\n",
    "#include \"xwidgets/xaudio.hpp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::ann;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace ens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::rl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the state and action space.\n",
    "DiscreteActionEnv::State::dimension = 8;\n",
    "DiscreteActionEnv::Action::size = 4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the network.\n",
    "FFN<MeanSquaredError<>, GaussianInitialization> network(\n",
    "    MeanSquaredError<>(), GaussianInitialization(0, 1));\n",
    "network.Add<Linear<>>(DiscreteActionEnv::State::dimension, 128);\n",
    "network.Add<ReLULayer<>>();\n",
    "network.Add<Linear<>>(128, DiscreteActionEnv::Action::size);\n",
    "\n",
    "SimpleDQN<> model(network);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the policy and replay method.\n",
    "GreedyPolicy<DiscreteActionEnv> policy(1.0, 2000, 0.1, 0.99);\n",
    "RandomReplay<DiscreteActionEnv> replayMethod(64, 100000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up training configurations.\n",
    "TrainingConfig config;\n",
    "config.ExplorationSteps() = 100;\n",
    "config.DoubleQLearning() = false;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up DQN agent.\n",
    "QLearning<DiscreteActionEnv, decltype(model), AdamUpdate, decltype(policy)>\n",
    "    agent(config, model, policy, replayMethod);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the gym training environment.\n",
    "gym::Environment env(\"gym.kurg.org\", \"4040\", \"LunarLander-v2\");\n",
    "\n",
    "// Initializing training variables.\n",
    "std::vector<double> returnList;\n",
    "size_t episodes = 0;\n",
    "bool converged = true;\n",
    "\n",
    "// The number of episode returns to keep track of.\n",
    "size_t consecutiveEpisodes = 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Function to train the agent on the gym implementation of LunarLander environment.\n",
    "void train(const size_t numSteps)\n",
    "{\n",
    "  agent.Deterministic() = false;\n",
    "  std::cout << \"Training for \" << numSteps << \" steps.\" << std::endl;\n",
    "  while (agent.TotalSteps() < numSteps)\n",
    "  {\n",
    "    double episodeReturn = 0;\n",
    "    env.reset();\n",
    "    do\n",
    "    {\n",
    "      agent.State().Data() = env.observation;\n",
    "      agent.SelectAction();\n",
    "      arma::mat action = {double(agent.Action().action)};\n",
    "\n",
    "      env.step(action);\n",
    "      DiscreteActionEnv::State nextState;\n",
    "      nextState.Data() = env.observation;\n",
    "\n",
    "      replayMethod.Store(agent.State(), agent.Action(), env.reward, nextState,\n",
    "          env.done, 0.99);\n",
    "      episodeReturn += env.reward;\n",
    "      agent.TotalSteps()++;\n",
    "      if (agent.Deterministic() || agent.TotalSteps() < config.ExplorationSteps())\n",
    "        continue;\n",
    "      agent.TrainAgent();\n",
    "    } while (!env.done);\n",
    "    returnList.push_back(episodeReturn);\n",
    "    episodes += 1;\n",
    "\n",
    "    if (returnList.size() > consecutiveEpisodes)\n",
    "      returnList.erase(returnList.begin());\n",
    "        \n",
    "    double averageReturn = std::accumulate(returnList.begin(),\n",
    "                                           returnList.end(), 0.0) /\n",
    "                           returnList.size();\n",
    "    if(episodes % 5 == 0)\n",
    "    {\n",
    "      std::cout << \"Avg return in last \" << returnList.size()\n",
    "          << \" episodes: \" << averageReturn\n",
    "          << \"\\t\" << episodes\n",
    "          << \"th episode return: \" << episodeReturn\n",
    "          << \"\\t Steps: \" << agent.TotalSteps() << std::endl;\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let the training begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps.\n",
      "Avg return in last 5 episodes: -284.722\t5th episode return: -488.03\t Steps: 468\n",
      "Avg return in last 10 episodes: -222.597\t10th episode return: -50.4032\t Steps: 986\n",
      "Avg return in last 15 episodes: -214.767\t15th episode return: -257.857\t Steps: 1543\n",
      "Avg return in last 20 episodes: -237.452\t20th episode return: -299.66\t Steps: 2182\n",
      "Avg return in last 25 episodes: -229.018\t25th episode return: -69.8594\t Steps: 2710\n",
      "Avg return in last 30 episodes: -228.616\t30th episode return: -126.706\t Steps: 3442\n",
      "Avg return in last 35 episodes: -219.267\t35th episode return: -175.728\t Steps: 4037\n",
      "Avg return in last 40 episodes: -202.473\t40th episode return: -61.6095\t Steps: 4719\n",
      "Avg return in last 45 episodes: -193.164\t45th episode return: -225.985\t Steps: 5232\n",
      "Avg return in last 50 episodes: -194.389\t50th episode return: -275.596\t Steps: 6061\n",
      "Avg return in last 50 episodes: -173.883\t55th episode return: -97.9969\t Steps: 7916\n",
      "Avg return in last 50 episodes: -175.869\t60th episode return: -96.9271\t Steps: 8860\n",
      "Avg return in last 50 episodes: -166.472\t65th episode return: 118.87\t Steps: 10819\n"
     ]
    }
   ],
   "source": [
    "// Training the agent for a total of at least 10000 steps.\n",
    "train(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total steps: 165\t Total reward: -51.3545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20070cd5b03f49249cd41a9c005af440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Deterministic() = true;\n",
    "\n",
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment envTest(\"gym.kurg.org\", \"4040\", \"LunarLander-v2\");\n",
    "envTest.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {double(agent.Action().action)};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += envTest.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little more training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps.\n",
      "Avg return in last 50 episodes: -150.198\t70th episode return: -89.2092\t Steps: 13028\n",
      "Avg return in last 50 episodes: -155.74\t75th episode return: -23.7489\t Steps: 15029\n",
      "Avg return in last 50 episodes: -149.012\t80th episode return: -211.166\t Steps: 16341\n",
      "Avg return in last 50 episodes: -156.946\t85th episode return: -227.523\t Steps: 17199\n",
      "Avg return in last 50 episodes: -165.563\t90th episode return: -37.9676\t Steps: 18004\n",
      "Avg return in last 50 episodes: -162.041\t95th episode return: -39.4172\t Steps: 19212\n",
      "Avg return in last 50 episodes: -164.412\t100th episode return: -424.532\t Steps: 19981\n",
      "Avg return in last 50 episodes: -173.323\t105th episode return: -40.9453\t Steps: 22067\n",
      "Avg return in last 50 episodes: -161.326\t110th episode return: -150.18\t Steps: 24134\n",
      "Avg return in last 50 episodes: -161.814\t115th episode return: -159.1\t Steps: 26374\n",
      "Avg return in last 50 episodes: -156.914\t120th episode return: -278.149\t Steps: 27964\n",
      "Avg return in last 50 episodes: -164.541\t125th episode return: -18.0229\t Steps: 29049\n",
      "Avg return in last 50 episodes: -165.112\t130th episode return: -44.7807\t Steps: 30669\n",
      "Avg return in last 50 episodes: -152.749\t135th episode return: -5.52039\t Steps: 32498\n",
      "Avg return in last 50 episodes: -151.152\t140th episode return: -181.733\t Steps: 33290\n",
      "Avg return in last 50 episodes: -154.841\t145th episode return: -76.0916\t Steps: 34461\n",
      "Avg return in last 50 episodes: -145.631\t150th episode return: -268.096\t Steps: 36679\n",
      "Avg return in last 50 episodes: -140.865\t155th episode return: -224.352\t Steps: 38962\n",
      "Avg return in last 50 episodes: -149.099\t160th episode return: 32.8503\t Steps: 40415\n",
      "Avg return in last 50 episodes: -166.081\t165th episode return: -345.188\t Steps: 41365\n",
      "Avg return in last 50 episodes: -168.507\t170th episode return: -2.32903\t Steps: 42319\n",
      "Avg return in last 50 episodes: -159.061\t175th episode return: -279.514\t Steps: 43988\n",
      "Avg return in last 50 episodes: -147.82\t180th episode return: -145.936\t Steps: 45600\n",
      "Avg return in last 50 episodes: -163.363\t185th episode return: -359.342\t Steps: 46877\n",
      "Avg return in last 50 episodes: -155.136\t190th episode return: 114.854\t Steps: 49150\n",
      "Avg return in last 50 episodes: -153.039\t195th episode return: -9.81771\t Steps: 50288\n",
      "Avg return in last 50 episodes: -158.38\t200th episode return: -282.745\t Steps: 51375\n",
      "Avg return in last 50 episodes: -156.798\t205th episode return: 69.8839\t Steps: 53838\n",
      "Avg return in last 50 episodes: -163.093\t210th episode return: -355.809\t Steps: 55658\n",
      "Avg return in last 50 episodes: -150.375\t215th episode return: -232.851\t Steps: 57720\n",
      "Avg return in last 50 episodes: -152.227\t220th episode return: -72.3069\t Steps: 59657\n",
      "Avg return in last 50 episodes: -137.959\t225th episode return: -266.234\t Steps: 61246\n",
      "Avg return in last 50 episodes: -140.269\t230th episode return: -250.204\t Steps: 63513\n",
      "Avg return in last 50 episodes: -125.516\t235th episode return: -397.982\t Steps: 66431\n",
      "Avg return in last 50 episodes: -119.547\t240th episode return: 2.22265\t Steps: 68185\n",
      "Avg return in last 50 episodes: -113.76\t245th episode return: 18.3692\t Steps: 70181\n",
      "Avg return in last 50 episodes: -95.7064\t250th episode return: 235.291\t Steps: 73477\n",
      "Avg return in last 50 episodes: -81.8454\t255th episode return: -106.881\t Steps: 76288\n",
      "Avg return in last 50 episodes: -63.3723\t260th episode return: 9.42518\t Steps: 79212\n",
      "Avg return in last 50 episodes: -42.1539\t265th episode return: 183.974\t Steps: 83346\n",
      "Avg return in last 50 episodes: -26.4426\t270th episode return: -103.536\t Steps: 85621\n",
      "Avg return in last 50 episodes: -27.6972\t275th episode return: -236.068\t Steps: 86919\n",
      "Avg return in last 50 episodes: -20.8178\t280th episode return: -251.282\t Steps: 89798\n",
      "Avg return in last 50 episodes: -20.9476\t285th episode return: -171.831\t Steps: 91316\n",
      "Avg return in last 50 episodes: -16.5527\t290th episode return: 220.264\t Steps: 94216\n",
      "Avg return in last 50 episodes: -5.96564\t295th episode return: 85.4784\t Steps: 97154\n",
      "Avg return in last 50 episodes: -19.3297\t300th episode return: -6.2327\t Steps: 100256\n"
     ]
    }
   ],
   "source": [
    "// Training the same agent for a total of at least 100000 steps.\n",
    "train(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final agent testing!\n",
    "*Note*: If you don't find a satisfactory output, please rerun the cell below. It's not guaranteed that the agent will receive high rewards on all test runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total steps: 526\t Total reward: 234.908\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3484af834b04d55b351f9f930e88f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Deterministic() = true;\n",
    "\n",
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment envTest(\"gym.kurg.org\", \"4040\", \"LunarLander-v2\");\n",
    "envTest.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {double(agent.Action().action)};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += envTest.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
