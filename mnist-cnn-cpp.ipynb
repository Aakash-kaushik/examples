{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![mlpack-lab Image](https://img.shields.io/endpoint?url=https%3A%2F%2Flab.kurg.org%2Fstatus%2Fstatus.json)](https://lab.mlpack.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    " * @file mnist-cnn-cpp.ipynb\n",
    " *\n",
    " * An example of using Convolutional Neural Network (CNN) for\n",
    " * solving Digit Recognizer problem from Kaggle website.\n",
    " *\n",
    " * The full description of a problem as well as datasets for training\n",
    " * and testing are available here https://www.kaggle.com/c/digit-recognizer.\n",
    " */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: mnist-train.csv: Cannot utime: Operation not permitted\n",
      "tar: Exiting with failure status due to previous errors\n"
     ]
    }
   ],
   "source": [
    "!wget -c https://lab.mlpack.org/data/mnist.tar.gz -O - | tar -xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/core.hpp>\n",
    "#include <mlpack/core/data/split_data.hpp>\n",
    "#include <mlpack/methods/ann/layer/layer.hpp>\n",
    "#include <mlpack/methods/ann/ffn.hpp>\n",
    "#include <ensmallen.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::ann;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    " * Returns labels bases on predicted probability (or log of probability)\n",
    " * of classes.\n",
    " *\n",
    " * @param predOut matrix contains probabilities (or log of probability) of\n",
    " *     classes. Each row corresponds to a certain class, each column corresponds to a data point.\n",
    " * @return a row vector of data points classes. The classes starts from 1 to the number of rows in input matrix.\n",
    " */\n",
    "arma::Row<size_t> getLabels(const arma::mat& predOut)\n",
    "{\n",
    "  arma::Row<size_t> pred(predOut.n_cols);\n",
    "\n",
    "  // Class of a j-th data point is chosen to be the one with maximum value\n",
    "  // in j-th column plus 1 (since column's elements are numbered from 0).\n",
    "  for (size_t j = 0; j < predOut.n_cols; ++j)\n",
    "  {\n",
    "    pred(j) = arma::as_scalar(arma::find(\n",
    "        arma::max(predOut.col(j)) == predOut.col(j), 1)) + 1;\n",
    "  }\n",
    "\n",
    "  return pred;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    " * Returns the accuracy (percentage of correct answers).\n",
    " *\n",
    " * @param predLabels predicted labels of data points.\n",
    " * @param realY real labels (they are double because we usually read them from CSV file that contain many other double values).\n",
    " * @return percentage of correct answers.\n",
    " */\n",
    "double accuracy(arma::Row<size_t> predLabels, const arma::mat& realY)\n",
    "{\n",
    "  // Calculating how many predicted classes are coincide with real labels.\n",
    "  size_t success = 0;\n",
    "  for (size_t j = 0; j < realY.n_cols; j++) {\n",
    "    if (predLabels(j) == std::round(realY(j))) {\n",
    "      ++success;\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Calculating percentage of correctly classified data points.\n",
    "  return (double)success / (double)realY.n_cols * 100.0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    " * Saves prediction into specifically formated CSV file, suitable for\n",
    " * most Kaggle competitions.\n",
    " * @param filename the name of a file.\n",
    " * @param header the header in a CSV file.\n",
    " * @param predLabels predicted labels of data points. Classes of data points\n",
    " * are expected to start from 1. At the same time classes of data points in\n",
    " * the file are going to start from 0 (as Kaggle usually expects)\n",
    " */\n",
    "void save(const std::string filename, std::string header,\n",
    "  const arma::Row<size_t>& predLabels)\n",
    "{\n",
    "  std::ofstream out(filename);\n",
    "  out << header << std::endl;\n",
    "  for (size_t j = 0; j < predLabels.n_cols; ++j)\n",
    "  {\n",
    "    // j + 1 because Kaggle indexes start from 1\n",
    "    // pred - 1 because 1st class is 0, 2nd class is 1 and etc.\n",
    "    out << j + 1 << \",\" << std::round(predLabels(j)) - 1;\n",
    "    // To avoid an empty line in the end of the file.\n",
    "    if (j < predLabels.n_cols - 1)\n",
    "    {\n",
    "      out << std::endl;\n",
    "    }\n",
    "  }\n",
    "  out.close();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Dataset is randomly split into validation\n",
    "// and training parts with following ratio.\n",
    "constexpr double RATIO = 0.1;\n",
    "\n",
    "// Number of iteration per cycle.\n",
    "constexpr int ITERATIONS_PER_CYCLE = 10000;\n",
    "\n",
    "// Number of cycles.\n",
    "constexpr int CYCLES = 40;\n",
    "\n",
    "// Step size of the optimizer.\n",
    "constexpr double STEP_SIZE = 1.2e-3;\n",
    "\n",
    "// Number of data points in each iteration of SGD.\n",
    "constexpr int BATCH_SIZE = 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Labeled dataset that contains data for training is loaded from CSV file. Rows represent features, columns represent data points.\n",
    "arma::mat tempDataset;\n",
    "\n",
    "// The original file can be downloaded from https://www.kaggle.com/c/digit-recognizer/data\n",
    "data::Load(\"mnist-train.csv\", tempDataset, true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The original Kaggle dataset CSV file has headings for each column, so it's necessary to get rid of the first row. In Armadillo representation, this corresponds to the first column of our data matrix.\n",
    "arma::mat dataset = tempDataset.submat(0, 1, tempDataset.n_rows - 1, tempDataset.n_cols - 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Split the dataset into training and validation sets.\n",
    "arma::mat train, valid;\n",
    "data::Split(dataset, train, valid, RATIO);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The train and valid datasets contain both - the features as well as the\n",
    "// class labels. Split these into separate mats.\n",
    "const arma::mat trainX = train.submat(1, 0, train.n_rows - 1, train.n_cols - 1);\n",
    "const arma::mat validX = valid.submat(1, 0, valid.n_rows - 1, valid.n_cols - 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// According to NegativeLogLikelihood output layer of NN, labels should\n",
    "// specify class of a data point and be in the interval from 1 to\n",
    "// number of classes (in this case from 1 to 10).\n",
    "\n",
    "// Create labels for training and validatiion datasets.\n",
    "const arma::mat trainY = train.row(0) + 1;\n",
    "const arma::mat validY = valid.row(0) + 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Specify the NN model. NegativeLogLikelihood is the output layer that\n",
    "// is used for classification problem. RandomInitialization means that\n",
    "// initial weights are generated randomly in the interval from -1 to 1.\n",
    "FFN<NegativeLogLikelihood<>, RandomInitialization> model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Specify the model architecture.\n",
    "// In this example, the CNN architecture is chosen similar to LeNet-5.\n",
    "// The architecture follows a Conv-ReLU-Pool-Conv-ReLU-Pool-Dense schema. We\n",
    "// have used leaky ReLU activation instead of vanilla ReLU. Standard\n",
    "// max-pooling has been used for pooling. The first convolution uses 6 filters\n",
    "// of size 5x5 (and a stride of 1). The second convolution uses 16 filters of\n",
    "// size 5x5 (stride = 1). The final dense layer is connected to a softmax to\n",
    "// ensure that we get a valid probability distribution over the output classes\n",
    "\n",
    "// Layers schema.\n",
    "// 28x28x1 --- conv (6 filters of size 5x5. stride = 1) ---> 24x24x6\n",
    "// 24x24x6 --------------- Leaky ReLU ---------------------> 24x24x6\n",
    "// 24x24x6 --- max pooling (over 2x2 fields. stride = 2) --> 12x12x6\n",
    "// 12x12x6 --- conv (16 filters of size 5x5. stride = 1) --> 8x8x16\n",
    "// 8x8x16  --------------- Leaky ReLU ---------------------> 8x8x16\n",
    "// 8x8x16  --- max pooling (over 2x2 fields. stride = 2) --> 4x4x16\n",
    "// 4x4x16  ------------------- Dense ----------------------> 10\n",
    "\n",
    "// Add the first convolution layer.\n",
    "model.Add<Convolution<> >(\n",
    "  1,  // Number of input activation maps.\n",
    "  6,  // Number of output activation maps.\n",
    "  5,  // Filter width.\n",
    "  5,  // Filter height.\n",
    "  1,  // Stride along width.\n",
    "  1,  // Stride along height.\n",
    "  0,  // Padding width.\n",
    "  0,  // Padding height.\n",
    "  28, // Input width.\n",
    "  28  // Input height.\n",
    "  );\n",
    "\n",
    "// Add first ReLU.\n",
    "model.Add<LeakyReLU<> >();\n",
    "\n",
    "// Add first pooling layer. Pools over 2x2 fields in the input.\n",
    "model.Add<MaxPooling<> >(\n",
    "  2,  // Width of field.\n",
    "  2,  // Height of field.\n",
    "  2,  // Stride along width.\n",
    "  2,  // Stride along height.\n",
    "  true\n",
    "  );\n",
    "\n",
    "// Add the second convolution layer.\n",
    "model.Add<Convolution<> >(\n",
    "  6,  // Number of input activation maps.\n",
    "  16, // Number of output activation maps.\n",
    "  5,  // Filter width.\n",
    "  5,  // Filter height.\n",
    "  1,  // Stride along width.\n",
    "  1,  // Stride along height.\n",
    "  0,  // Padding width.\n",
    "  0,  // Padding height.\n",
    "  12, // Input width.\n",
    "  12  // Input height.\n",
    "  );\n",
    "\n",
    "// Add the second ReLU.\n",
    "model.Add<LeakyReLU<> >();\n",
    "\n",
    "// Add the second pooling layer.\n",
    "model.Add<MaxPooling<> >(2, 2, 2, 2, true);\n",
    "\n",
    "// Add the final dense layer.\n",
    "model.Add<Linear<> >(16*4*4, 10);\n",
    "model.Add<LogSoftMax<> >();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set parameters of Stochastic Gradient Descent (SGD) optimizer.\n",
    "ens::SGD<ens::AdamUpdate> optimizer(\n",
    "    // Step size of the optimizer.\n",
    "    STEP_SIZE,\n",
    "    // Batch size. Number of data points that are used in each iteration.\n",
    "    BATCH_SIZE,\n",
    "    // Max number of iterations.\n",
    "    ITERATIONS_PER_CYCLE,\n",
    "    // Tolerance, used as a stopping condition. Such a small value\n",
    "    // means we almost never stop by this condition, and continue gradient\n",
    "    // descent until the maximum number of iterations is reached.\n",
    "    1e-8,\n",
    "    // Shuffle. If optimizer should take random data points from the dataset at\n",
    "    // each iteration.\n",
    "    true,\n",
    "    // Adam update policy.\n",
    "    ens::AdamUpdate(1e-8, 0.9, 0.999));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 0:\tTraining Accuracy = 42.7672%,\tValidation Accuracy = 42.8571%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 1:\tTraining Accuracy = 60.9418%,\tValidation Accuracy = 60.7857%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 2:\tTraining Accuracy = 69.4762%,\tValidation Accuracy = 69.381%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 3:\tTraining Accuracy = 74.3201%,\tValidation Accuracy = 73.9286%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 4:\tTraining Accuracy = 77.5317%,\tValidation Accuracy = 76.3571%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 5:\tTraining Accuracy = 79.4683%,\tValidation Accuracy = 78.4762%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 6:\tTraining Accuracy = 81.1561%,\tValidation Accuracy = 80.4286%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 7:\tTraining Accuracy = 82.4259%,\tValidation Accuracy = 81.7857%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 8:\tTraining Accuracy = 83.3439%,\tValidation Accuracy = 82.5%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 9:\tTraining Accuracy = 84.2725%,\tValidation Accuracy = 83.5714%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 10:\tTraining Accuracy = 85.119%,\tValidation Accuracy = 84.4762%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 11:\tTraining Accuracy = 85.8545%,\tValidation Accuracy = 85.4524%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 12:\tTraining Accuracy = 86.6931%,\tValidation Accuracy = 86.2619%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 13:\tTraining Accuracy = 87.0212%,\tValidation Accuracy = 86.6429%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 14:\tTraining Accuracy = 87.6376%,\tValidation Accuracy = 87.2857%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 15:\tTraining Accuracy = 88.2037%,\tValidation Accuracy = 88.1429%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 16:\tTraining Accuracy = 88.7143%,\tValidation Accuracy = 88.6429%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 17:\tTraining Accuracy = 89.082%,\tValidation Accuracy = 88.7857%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 18:\tTraining Accuracy = 89.5794%,\tValidation Accuracy = 89.2381%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 19:\tTraining Accuracy = 89.8862%,\tValidation Accuracy = 89.5238%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 20:\tTraining Accuracy = 90.172%,\tValidation Accuracy = 89.619%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 21:\tTraining Accuracy = 90.5979%,\tValidation Accuracy = 90.0952%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 22:\tTraining Accuracy = 90.7725%,\tValidation Accuracy = 89.9524%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 23:\tTraining Accuracy = 91.1111%,\tValidation Accuracy = 90.4286%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 24:\tTraining Accuracy = 91.3069%,\tValidation Accuracy = 90.6905%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 25:\tTraining Accuracy = 91.5291%,\tValidation Accuracy = 90.7857%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 26:\tTraining Accuracy = 91.7037%,\tValidation Accuracy = 91.0238%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 27:\tTraining Accuracy = 91.9921%,\tValidation Accuracy = 91.2381%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 28:\tTraining Accuracy = 92.1111%,\tValidation Accuracy = 91.2381%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 29:\tTraining Accuracy = 92.2196%,\tValidation Accuracy = 91.4524%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 30:\tTraining Accuracy = 92.2169%,\tValidation Accuracy = 91.4048%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 31:\tTraining Accuracy = 92.4709%,\tValidation Accuracy = 91.5714%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 32:\tTraining Accuracy = 92.5026%,\tValidation Accuracy = 91.7143%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 33:\tTraining Accuracy = 92.5317%,\tValidation Accuracy = 91.5952%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 34:\tTraining Accuracy = 92.9233%,\tValidation Accuracy = 91.881%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 35:\tTraining Accuracy = 93.0185%,\tValidation Accuracy = 92%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 36:\tTraining Accuracy = 92.9947%,\tValidation Accuracy = 91.8095%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 37:\tTraining Accuracy = 93.0899%,\tValidation Accuracy = 91.8333%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 38:\tTraining Accuracy = 93.3201%,\tValidation Accuracy = 91.9048%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 39:\tTraining Accuracy = 93.6058%,\tValidation Accuracy = 92.1905%\n",
      "\u001b[0;33m[WARN ] \u001b[0mThe optimizer's maximum number of iterations is less than the size of the dataset; the optimizer will not pass over the entire dataset. To fix this, modify the maximum number of iterations to be at least equal to the number of points of your dataset (37800).\n",
      "Epoch 40:\tTraining Accuracy = 93.7143%,\tValidation Accuracy = 92.4048%\n"
     ]
    }
   ],
   "source": [
    "for (int i = 0; i <= CYCLES; i++)\n",
    "{\n",
    "  // Train the CNN model. If this is the first iteration, weights are\n",
    "  // randomly initialized between -1 and 1. Otherwise, the values of weights\n",
    "  // from the previous iteration are used.\n",
    "  model.Train(trainX, trainY, optimizer);\n",
    "\n",
    "  // Don't reset optimizers parameters between cycles.\n",
    "  optimizer.ResetPolicy() = false;\n",
    "\n",
    "  // Matrix to store the predictions on train and validation datasets.\n",
    "  arma::mat predOut;\n",
    "  // Get predictions on training data points.\n",
    "  model.Predict(trainX, predOut);\n",
    "  // Calculate accuracy on training data points.\n",
    "  arma::Row<size_t> predLabels = getLabels(predOut);\n",
    "  double trainAccuracy = accuracy(predLabels, trainY);\n",
    "  // Get predictions on validating data points.\n",
    "  model.Predict(validX, predOut);\n",
    "  // Calculate accuracy on validating data points.\n",
    "  predLabels = getLabels(predOut);\n",
    "  double validAccuracy = accuracy(predLabels, validY);\n",
    "\n",
    "  std::cout << \"Epoch \" << i << \":\\tTraining Accuracy = \"<< trainAccuracy<< \"%,\" <<\"\\tValidation Accuracy = \"<< validAccuracy << \"%\" << std::endl;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting ...\n"
     ]
    }
   ],
   "source": [
    "std::cout << \"Predicting ...\" << std::endl;\n",
    "// Load test dataset. The original file could be download from https://www.kaggle.com/c/digit-recognizer/data.\n",
    "data::Load(\"mnist-test.csv\", tempDataset, true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predicted labels to results.csv.\n"
     ]
    }
   ],
   "source": [
    "// As before, it's necessary to get rid of column headings.\n",
    "arma::mat testX = tempDataset.submat(0, 1, tempDataset.n_rows - 1, tempDataset.n_cols - 1);\n",
    "// Matrix to store the predictions on test dataset.\n",
    "arma::mat testPredOut;\n",
    "\n",
    "// Get predictions on test data points.\n",
    "model.Predict(testX, testPredOut);\n",
    "// Generate labels for the test dataset.\n",
    "arma::Row<size_t> testPred = getLabels(testPredOut);\n",
    "std::cout << \"Saving predicted labels to results.csv.\"<< std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results were saved to kaggel-results.csv. This file can be uploaded to https://www.kaggle.com/c/digit-recognizer/submissions.\n"
     ]
    }
   ],
   "source": [
    "// Saving results into Kaggle compatibe CSV file.\n",
    "save(\"kaggel-results.csv\", \"ImageId,Label\", testPred);\n",
    "std::cout << \"Results were saved to kaggel-results.csv. This file can be uploaded to \"\n",
    "    << \"https://www.kaggle.com/c/digit-recognizer/submissions.\" << std::endl;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++11",
   "language": "C++11",
   "name": "xcpp11"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
