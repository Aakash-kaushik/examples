{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Regression\n",
    "In this example, we will create a neural network mlpack for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup\n",
    "Include all libraries required to implement this tutorial. These mainly include files from mlpack, ensmallen and armadillo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/prereqs.hpp>\n",
    "#include <mlpack/core.hpp>\n",
    "#include <mlpack/methods/ann/loss_functions/mean_squared_error.hpp>\n",
    "#include <mlpack/core/data/scaler_methods/min_max_scaler.hpp>\n",
    "#include <mlpack/methods/ann/layer/layer.hpp>\n",
    "#include <mlpack/core/data/split_data.hpp>\n",
    "#include <mlpack/methods/ann/ffn.hpp>\n",
    "#include <mlpack/methods/ann/init_rules/he_init.hpp>\n",
    "#include <ensmallen.hpp>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some convenient namespaces to simplify the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::ann;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace ens;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set Model and Training parameters.\n",
    "Set the training parameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Testing data is taken from the dataset in this ratio.\n",
    "constexpr double RATIO = 0.1; //25%\n",
    "\n",
    "//! - H1: The number of neurons in the 1st layer.\n",
    "constexpr int H1 = 64;\n",
    "//! - H2: The number of neurons in the 2nd layer.\n",
    "constexpr int H2 = 128;\n",
    "//! - H3: The number of neurons in the 3rd layer.\n",
    "constexpr int H3 = 64;\n",
    "\n",
    "// Number of epochs for training.\n",
    "const int EPOCHS = 300;\n",
    "//! - STEP_SIZE: Step size of the optimizer.\n",
    "constexpr double STEP_SIZE = 5e-2;\n",
    "//! - BATCH_SIZE: Number of data points in each iteration of SGD.\n",
    "constexpr int BATCH_SIZE = 32;\n",
    "//! - STOP_TOLERANCE: Stop tolerance;\n",
    "// A very small number implies that we do all iterations.\n",
    "constexpr double STOP_TOLERANCE = 1e-8;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set paths for the dataset, trained model and final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "//! Path to the dataset used for training and testing.\n",
    "const std::string datasetPath = \"./bodyfat.tsv\";\n",
    "// File for saving the model.\n",
    "const std::string modelFile = \"nn_regressor.bin\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Loading the Dataset.\n",
    "The bodyfat dataset contains estimates of the percentage of body fat determined by\n",
    "underwater weighing and various body circumference measurements for 252 men. Accurate measurement of body fat is very expensive,but by using machine learning it is possible to calculate a prediction with good accuracy by just using some low cost\n",
    "measurements of the body. The columns in the dataset are the following:\n",
    "\n",
    "* Percent body fat (%) => this is the decision column (what we want to get from the model).\n",
    "* Age (years)\n",
    "* Weight (lbs)\n",
    "* Height (inches)\n",
    "* Neck circumference (cm)\n",
    "* Chest circumference (cm)\n",
    "* Abdomen 2 circumference (cm)\n",
    "* Hip circumference (cm)\n",
    "* Thigh circumference (cm)\n",
    "* Knee circumference (cm)\n",
    "* Ankle circumference (cm)\n",
    "* Biceps (extended) circumference (cm)\n",
    "* Forearm circumference (cm)\n",
    "* Wrist circumference (cm)\n",
    "* Density determined from underwater weighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arma::mat dataset;\n",
    "// In Armadillo columns represent data points, rows represent features.\n",
    "data::Load(datasetPath, dataset, true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preprocess the dataset.\n",
    "Split the data into training and validation set. We will also scale the data to increase stability in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IncrementalExecutor::executeFunction: symbol '__emutls_v._ZN4arma23arma_rng_cxx11_instanceE' unresolved while linking function '_GLOBAL__sub_I_cling_module_14'!\n"
     ]
    }
   ],
   "source": [
    "// Split the dataset into training and validation sets.\n",
    "arma::mat trainData, validData;\n",
    "data::Split(dataset, trainData, validData, RATIO);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into input features and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The train and valid datasets contain both - the features as well as the\n",
    "// prediction. Split these into separate matrices.\n",
    "arma::mat trainX = trainData.submat(1, 0, trainData.n_rows - 1,\n",
    "                                    trainData.n_cols - 1);\n",
    "arma::mat validX = validData.submat(1, 0, validData.n_rows - 1,\n",
    "                                    validData.n_cols - 1);\n",
    "\n",
    "// Create prediction data for training and validatiion datasets.\n",
    "arma::mat trainY = trainData.row(0);\n",
    "arma::mat validY = validData.row(0);\n",
    "\n",
    "// Scale all data into the range (0, 1) for increased numerical stability.\n",
    "data::MinMaxScaler scale;\n",
    "// Fit scaler only on training data.\n",
    "scale.Fit(trainX);\n",
    "scale.Transform(trainX, trainX);\n",
    "scale.Transform(validX, validX);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create the Model\n",
    "Specifying the NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// This intermediate layer is needed for connection between input\n",
    "// data and the next LeakyReLU layer.\n",
    "// Parameters specify the number of input features and number of\n",
    "// neurons in the next layer.\n",
    "model.Add<Linear<>>(trainX.n_rows, H1);\n",
    "// Activation layer:\n",
    "model.Add<LeakyReLU<>>();\n",
    "// Connection layer between two activation layers.\n",
    "model.Add<Linear<>>(H1, H2);\n",
    "// Activation layer.\n",
    "model.Add<LeakyReLU<>>();\n",
    "// Connection layer.\n",
    "model.Add<Linear<>>(H2, H3);\n",
    "// Activation layer.\n",
    "model.Add<LeakyReLU<>>();\n",
    "// Connection layer => output.\n",
    "// The output of one neuron is the regression output for one record.\n",
    "model.Add<Linear<>>(H3, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training the model.\n",
    "We will use ensmallen to create the optimizer and train the model. For more details refer to the [documentation](https://www.ensmallen.org/docs.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens::Adam optimizer(\n",
    "    STEP_SIZE, // Step size of the optimizer.\n",
    "    BATCH_SIZE, // Batch size. Number of data points that are used in each iteration.\n",
    "    0.9, // Exponential decay rate for the first moment estimates.\n",
    "    0.999, // Exponential decay rate for the weighted infinity norm estimates.\n",
    "    1e-8, // Value used to initialise the mean squared gradient parameter.\n",
    "    trainData.n_cols * EPOCHS, // Max number of iterations.\n",
    "    1e-8,// Tolerance.\n",
    "    true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use ensmallen callbacks to train the model. We will be using Adam optimizer. To stop the training when the loss stops decreasing or doesn't show any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Train the model.\n",
    "model.Train(trainX,\n",
    "            trainY,\n",
    "            optimizer,\n",
    "            // PrintLoss Callback prints loss for each epoch.\n",
    "            ens::PrintLoss(),\n",
    "            // Progressbar Callback prints progress bar for each epoch.\n",
    "            // Here 40 signifies width of progress bar.\n",
    "            ens::ProgressBar(40),\n",
    "            // Stops the optimization process if the loss stops decreasing\n",
    "            // or no improvement has been made. This will terminate the\n",
    "            // optimization once we obtain a minima on training set.\n",
    "            ens::EarlyStopAtMinLoss(20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Running Inference\n",
    "Get predictions on validation dataset and test the quality of our model by calculating Mean Squared Error on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create predictions on the dataset.\n",
    "arma::mat predOut;\n",
    "modelP.Predict(validX, predOut);\n",
    "\n",
    "// Calculate MSE loss on predictions.\n",
    "double validMSE = metric::SquaredEuclideanDistance::Evaluate(predOut, validY) / (validY.n_elem);\n",
    "std::cout << \"Mean Squared Error on Prediction data points: \" << validMSE << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Loading and Saving Models\n",
    "In the real world, we won't be training the model from scratch everytime we need to run inference.\n",
    "We will save the model once and load it as many times as we want for either training or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data::Save(modelFile, \"NNRegressor\", model);\n",
    "FFN<MeanSquaredError<>, HeInitialization> modelP;\n",
    "// Load weights into the model.\n",
    "data::Load(modelFile, \"NNRegressor\", modelP);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++11",
   "language": "C++11",
   "name": "xcpp11"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
