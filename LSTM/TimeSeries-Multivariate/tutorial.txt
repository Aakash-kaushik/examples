/*!
@file tutorial.txt
@author Mehul Kumar Nirala
@brief Tutorial on Multivariate Time Series using RNN.

@page rnntutorial LSTM Multivariate Time Series

@section intro_lstmtut Introduction

We want to use the power of the LSTM in Google stock prediction using time series. We will use mlpack and Recurrent Neural Network(RNN).
I have downloaded the Google stock prices for past 3 years from https://www.nasdaq.com/symbol/goog/historical to a csv file Google2016-2019.csv

@section toc_lstmtut Table of Contents

This tutorial is split into the following sections:

 - \ref intro_lstmtut
 - \ref toc_lstmtut
 - \ref data_lstmtut
 - \ref model_lstmtut
 - \ref training_lstmtut
 - \ref results_lstmtut

@section data_lstmtut Time Series data
 Initially we normalize the input data using MinMaxScaler so that all the input features are on the scale from 0 to 1.

@code
  template<typename DataType = arma::mat>
  DataType MinMaxScaler(DataType& dataset)
  {
    arma::vec maxValues = arma::max(dataset, 1 /* for each dimension */);
    arma::vec minValues = arma::min(dataset, 1 /* for each dimension */);

    arma::vec rangeValues = maxValues - minValues;

    // Add a very small value if there are any zeros.
    rangeValues += 1e-25;

    dataset -= arma::repmat(minValues , 1, dataset.n_cols);
    dataset /= arma::repmat(rangeValues , 1, dataset.n_cols);
    return dataset;
  }
  ...
  // Scale data for increased numerical stability.
  dataset = MinMaxScaler(dataset);
@endcode

 * This is a time series problem. If we need to predict the Google stock prices correctly then we need to consider the volume of the stocks traded from the previous days as well as the average stock prices from previous days.
 * We will be creating the data that will go back to 25 business days in past for the prediction.
 * Also, we will take 30 % of the latest data as our test dataset.
 * For RNN LSTM to predict the data we need to convert the input data.
 * Input data is in the form: [Volume of stocks traded, Opening stock price, Closing stock price, Min stock price, Max stock price] and we need to create a time series data.

The time series data for today should contain the [Volume of stocks traded, Opening stock price, Closing stock price, Min stock price, Max stock price] for past 25 days and the target variable will be Google’s stock price today and so on.
As the stock price prediction is based on multiple input features, it is a multivariate regression problem.
We loop through all the samples and for each day we go back 25 business days in the past and add the volume of the stocks traded an average stock price.

@code
  /*
   * The time series data for today should contain
   * the [Volume of stocks traded, Average stock price]
   * for past 'rho' days and the target variable will be Google’s
   * stock price today (high, low) and so on.
   */
  template<typename InputDataType = arma::mat,
       typename DataType = arma::cube,
       typename LabelType = arma::cube>
  void CreateTimeSeriesData(InputDataType dataset, DataType& X, LabelType& y, size_t rho)
  {
    for(size_t i = 0;i<dataset.n_cols - rho - 1 ;i++)
    {
      X.subcube(span(), span(i), span()) = dataset.submat(span(), span(i, i+rho-1));
      y.subcube(span(), span(i), span()) = dataset.submat(span(3,4), span(i, i+rho-1));
    }
  }
@endcode

@section model_lstmtut LSTM Model

We add 30 RNN cells that will be stacked one after the other in the RNN, implementing an efficient stacked RNN. Finally, the output will have only one unit as this is a regression problem.

@code
  // No of timesteps to look in RNN.
  const int rho = 25;
  size_t inputSize = 5, outputSize = 2;

  // RNN model.
  RNN<MeanSquaredError<>,HeInitialization> model(rho);
  model.Add<IdentityLayer<> >();
  model.Add<LSTM<> > (inputSize, outputSize, maxRho);
  model.Add<Dropout<> >(0.5);
  model.Add<LeakyReLU<> >();
  model.Add<LSTM<> > (outputSize, outputSize, maxRho);
  model.Add<Dropout<> >(0.5);
  model.Add<LeakyReLU<> >();
  model.Add<LSTM<> > (outputSize, outputSize, maxRho);
  model.Add<Linear<> >(outputSize, outputSize);

@endcode

Setting parameters Stochastic Gradient Descent (SGD) optimizer.
@code

  // Setting parameters Stochastic Gradient Descent (SGD) optimizer.
  SGD<AdamUpdate> optimizer(
    STEP_SIZE, // Step size of the optimizer.
    BATCH_SIZE, // Batch size. Number of data points that are used in each iteration.
    ITERATIONS_PER_EPOCH, // Max number of iterations.
    1e-8,// Tolerance.
    true,// Shuffle.
    AdamUpdate(1e-8, 0.9, 0.999)// Adam update policy.
  );

@endcode

@section training_lstmtut Training the model

@code
  cout << "Training ..." << endl;
  // Cycles for monitoring the process of a solution.
  for (int i = 0; i < EPOCH; i++)
  {
    // Train neural network. If this is the first iteration, weights are
    // random, using current values as starting point otherwise.
    model.Train(trainX, trainY, optimizer);

    // Don't reset optimizer's parameters between cycles.
    optimizer.ResetPolicy() = false;

    cube predOut;
    // Getting predictions on test data points.
    model.Predict(testX, predOut);

    // Calculating mse on test data points.
    double testMSE = MSE(predOut,testY);
    cout << i+1<< " - Mean Squared Error := "<< testMSE <<   endl;
  }
@endcode

@section results_lstmtut Results

Mean Squared error upto 25 iterations.
 1 - Mean Squared Error := 0.474642
 2 - Mean Squared Error := 0.472394
 3 - Mean Squared Error := 0.470225
 4 - Mean Squared Error := 0.468131
 5 - Mean Squared Error := 0.466106
 6 - Mean Squared Error := 0.464155
 7 - Mean Squared Error := 0.462268
 8 - Mean Squared Error := 0.460441
 9 - Mean Squared Error := 0.458667
10 - Mean Squared Error := 0.456942
11 - Mean Squared Error := 0.455263
12 - Mean Squared Error := 0.453628
13 - Mean Squared Error := 0.452037
14 - Mean Squared Error := 0.450487
15 - Mean Squared Error := 0.44897
16 - Mean Squared Error := 0.447486
17 - Mean Squared Error := 0.446029
18 - Mean Squared Error := 0.444597
19 - Mean Squared Error := 0.443195
20 - Mean Squared Error := 0.441819
21 - Mean Squared Error := 0.440465
22 - Mean Squared Error := 0.43913
23 - Mean Squared Error := 0.437834
24 - Mean Squared Error := 0.43656
25 - Mean Squared Error := 0.435301
....

*/