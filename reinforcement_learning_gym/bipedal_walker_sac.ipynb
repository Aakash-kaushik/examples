{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://lab.mlpack.org/v2/gh/mlpack/examples/master?urlpath=lab%2Ftree%2Fq_learning%2Fbipedal_walker_sac.ipynb)\n",
    "\n",
    "You can easily run this notebook at https://lab.mlpack.org/\n",
    "\n",
    "Here, we train a [Soft Actor-Critic](https://arxiv.org/abs/1801.01290) agent to get high scores for the [Bipedal Walker](https://gym.openai.com/envs/BipedalWalker-v2/) environment. \n",
    "\n",
    "We make the agent train and test on OpenAI Gym toolkit's GUI interface provided through a distributed infrastructure (TCP API). More details can be found [here](https://github.com/zoq/gym_tcp_api).\n",
    "\n",
    "A video of the trained agent can be seen in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including necessary libraries and namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/core.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/methods/ann/ffn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/sac.hpp>\n",
    "#include <mlpack/methods/ann/loss_functions/empty_loss.hpp>\n",
    "#include <mlpack/methods/ann/init_rules/gaussian_init.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/environment/env_type.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/training_config.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to run the agent on gym's environment (provided externally) for testing.\n",
    "#include <gym/environment.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to generate and display a video of the trained agent.\n",
    "#include \"xwidgets/ximage.hpp\"\n",
    "#include \"xwidgets/xvideo.hpp\"\n",
    "#include \"xwidgets/xaudio.hpp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::ann;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace ens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::rl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the state and action space.\n",
    "ContinuousActionEnv::State::dimension = 24;\n",
    "ContinuousActionEnv::Action::size = 4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the actor and critic networks.\n",
    "FFN<EmptyLoss<>, GaussianInitialization>\n",
    "    policyNetwork(EmptyLoss<>(), GaussianInitialization(0, 0.1));\n",
    "policyNetwork.Add(new Linear<>(ContinuousActionEnv::State::dimension, 64));\n",
    "policyNetwork.Add(new ReLULayer<>());\n",
    "policyNetwork.Add(new Linear<>(64, ContinuousActionEnv::Action::size));\n",
    "policyNetwork.Add(new TanHLayer<>());\n",
    "\n",
    "FFN<EmptyLoss<>, GaussianInitialization>\n",
    "    qNetwork(EmptyLoss<>(), GaussianInitialization(0, 0.1));\n",
    "qNetwork.Add(new Linear<>(ContinuousActionEnv::State::dimension +\n",
    "                          ContinuousActionEnv::Action::size, 64));\n",
    "qNetwork.Add(new ReLULayer<>());\n",
    "qNetwork.Add(new Linear<>(64, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the policy method.\n",
    "RandomReplay<ContinuousActionEnv> replayMethod(32, 10000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up training configurations.\n",
    "TrainingConfig config;\n",
    "config.ExplorationSteps() = 3200;\n",
    "config.TargetNetworkSyncInterval() = 1;\n",
    "config.UpdateInterval() = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up Soft actor-critic agent.\n",
    "SAC<ContinuousActionEnv, decltype(qNetwork), decltype(policyNetwork), AdamUpdate>\n",
    "    agent(config, qNetwork, policyNetwork, replayMethod);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the gym training environment.\n",
    "gym::Environment env(\"gym.kurg.org\", \"4040\", \"BipedalWalker-v3\");\n",
    "\n",
    "// Initializing training variables.\n",
    "std::vector<double> returnList;\n",
    "size_t episodes = 0;\n",
    "bool converged = true;\n",
    "\n",
    "// The number of episode returns to keep track of.\n",
    "size_t consecutiveEpisodes = 25;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total steps: 106\t Total reward: -93.3435\n",
      "https://gym.kurg.org/a5eb46caf7934/output.webm"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6baaa44ffea47c2a3279f78feeb4849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Deterministic() = true;\n",
    "\n",
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment envTest(\"gym.kurg.org\", \"4040\", \"BipedalWalker-v3\");\n",
    "envTest.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {agent.Action().action};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += envTest.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "std::cout << url;\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Function to train the agent on the Bipedal Walker gym environment.\n",
    "void train(const size_t numSteps)\n",
    "{\n",
    "  agent.Deterministic() = false;\n",
    "  std::cout << \"Training for \" << numSteps << \" steps.\" << std::endl;\n",
    "  while (agent.TotalSteps() < numSteps)\n",
    "  {\n",
    "    double episodeReturn = 0;\n",
    "    env.reset();\n",
    "    size_t steps = 0;\n",
    "    do\n",
    "    {\n",
    "      agent.State().Data() = env.observation;\n",
    "      agent.SelectAction();\n",
    "      arma::mat action = {agent.Action().action};\n",
    "\n",
    "      env.step(action);\n",
    "      ContinuousActionEnv::State nextState;\n",
    "      nextState.Data() = env.observation;\n",
    "\n",
    "      replayMethod.Store(agent.State(), agent.Action(), env.reward, nextState,\n",
    "          env.done, 0.99);\n",
    "      episodeReturn += env.reward;\n",
    "      agent.TotalSteps()++;\n",
    "      steps++;\n",
    "      if (agent.Deterministic() || agent.TotalSteps() < config.ExplorationSteps())\n",
    "        continue;\n",
    "      for (size_t i = 0; i < config.UpdateInterval(); i++)\n",
    "        agent.Update();\n",
    "    } while (!env.done);\n",
    "    returnList.push_back(episodeReturn);\n",
    "    episodes += 1;\n",
    "\n",
    "    if (returnList.size() > consecutiveEpisodes)\n",
    "      returnList.erase(returnList.begin());\n",
    "        \n",
    "    double averageReturn = std::accumulate(returnList.begin(),\n",
    "                                           returnList.end(), 0.0) /\n",
    "                           returnList.size();\n",
    "    if(episodes % 1 == 0)\n",
    "    {\n",
    "      std::cout << \"Avg return for \" << returnList.size()\n",
    "          << \" episodes: \" << averageReturn\n",
    "          << \"\\t Episode return: \" << episodeReturn\n",
    "          << \"\\t steps: \" << steps\n",
    "          << \"\\t Total steps: \" << agent.TotalSteps() << std::endl;\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let the training begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps.\n",
      "Avg return for 1 episodes: -22.4558\t Episode return: -22.4558\t steps: 1600\t Total steps: 1600\n",
      "Avg return for 2 episodes: -71.2708\t Episode return: -120.086\t steps: 142\t Total steps: 1742\n",
      "Avg return for 3 episodes: -78.7429\t Episode return: -93.6872\t steps: 87\t Total steps: 1829\n",
      "Avg return for 4 episodes: -82.537\t Episode return: -93.9192\t steps: 90\t Total steps: 1919\n",
      "Avg return for 5 episodes: -93.9891\t Episode return: -139.798\t steps: 1389\t Total steps: 3308\n",
      "Avg return for 6 episodes: -98.9828\t Episode return: -123.951\t steps: 64\t Total steps: 3372\n",
      "Avg return for 7 episodes: -102.657\t Episode return: -124.699\t steps: 78\t Total steps: 3450\n",
      "Avg return for 8 episodes: -102.428\t Episode return: -100.828\t steps: 70\t Total steps: 3520\n",
      "Avg return for 9 episodes: -102.054\t Episode return: -99.0604\t steps: 69\t Total steps: 3589\n",
      "Avg return for 10 episodes: -102.289\t Episode return: -104.402\t steps: 74\t Total steps: 3663\n",
      "Avg return for 11 episodes: -102.715\t Episode return: -106.982\t steps: 84\t Total steps: 3747\n",
      "Avg return for 12 episodes: -110.792\t Episode return: -199.636\t steps: 1280\t Total steps: 5027\n",
      "Avg return for 13 episodes: -110.528\t Episode return: -107.364\t steps: 58\t Total steps: 5085\n",
      "Avg return for 14 episodes: -110.644\t Episode return: -112.146\t steps: 60\t Total steps: 5145\n",
      "Avg return for 15 episodes: -111.534\t Episode return: -123.993\t steps: 1600\t Total steps: 6745\n",
      "Avg return for 16 episodes: -111.343\t Episode return: -108.476\t steps: 51\t Total steps: 6796\n",
      "Avg return for 17 episodes: -111.279\t Episode return: -110.269\t steps: 54\t Total steps: 6850\n",
      "Avg return for 18 episodes: -110.811\t Episode return: -102.84\t steps: 64\t Total steps: 6914\n",
      "Avg return for 19 episodes: -110.57\t Episode return: -106.231\t steps: 48\t Total steps: 6962\n",
      "Avg return for 20 episodes: -110.355\t Episode return: -106.28\t steps: 46\t Total steps: 7008\n",
      "Avg return for 21 episodes: -110.163\t Episode return: -106.323\t steps: 47\t Total steps: 7055\n",
      "Avg return for 22 episodes: -109.982\t Episode return: -106.188\t steps: 46\t Total steps: 7101\n",
      "Avg return for 23 episodes: -109.799\t Episode return: -105.761\t steps: 46\t Total steps: 7147\n",
      "Avg return for 24 episodes: -109.624\t Episode return: -105.59\t steps: 48\t Total steps: 7195\n",
      "Avg return for 25 episodes: -109.476\t Episode return: -105.931\t steps: 47\t Total steps: 7242\n",
      "Avg return for 25 episodes: -112.857\t Episode return: -106.977\t steps: 48\t Total steps: 7290\n",
      "Avg return for 25 episodes: -112.429\t Episode return: -109.385\t steps: 56\t Total steps: 7346\n",
      "Avg return for 25 episodes: -114.04\t Episode return: -133.97\t steps: 145\t Total steps: 7491\n",
      "Avg return for 25 episodes: -120.641\t Episode return: -258.953\t steps: 1485\t Total steps: 8976\n",
      "Avg return for 25 episodes: -120.846\t Episode return: -144.915\t steps: 237\t Total steps: 9213\n",
      "Avg return for 25 episodes: -121.239\t Episode return: -133.779\t steps: 234\t Total steps: 9447\n",
      "Avg return for 25 episodes: -120.757\t Episode return: -112.651\t steps: 102\t Total steps: 9549\n",
      "Avg return for 25 episodes: -121.789\t Episode return: -126.62\t steps: 91\t Total steps: 9640\n",
      "Avg return for 25 episodes: -122.486\t Episode return: -116.488\t steps: 66\t Total steps: 9706\n",
      "Avg return for 25 episodes: -123.047\t Episode return: -118.437\t steps: 73\t Total steps: 9779\n",
      "Avg return for 25 episodes: -123.508\t Episode return: -118.489\t steps: 75\t Total steps: 9854\n",
      "Avg return for 25 episodes: -120.509\t Episode return: -124.682\t steps: 124\t Total steps: 9978\n",
      "Avg return for 25 episodes: -120.662\t Episode return: -111.174\t steps: 40\t Total steps: 10018\n",
      "Avg return for 25 episodes: -120.844\t Episode return: -116.706\t steps: 66\t Total steps: 10084\n",
      "Avg return for 25 episodes: -120.905\t Episode return: -125.516\t steps: 84\t Total steps: 10168\n",
      "Avg return for 25 episodes: -121.322\t Episode return: -118.901\t steps: 70\t Total steps: 10238\n",
      "Avg return for 25 episodes: -121.794\t Episode return: -122.065\t steps: 82\t Total steps: 10320\n",
      "Avg return for 25 episodes: -122.388\t Episode return: -117.7\t steps: 61\t Total steps: 10381\n",
      "Avg return for 25 episodes: -123.064\t Episode return: -123.132\t steps: 92\t Total steps: 10473\n",
      "Avg return for 25 episodes: -123.609\t Episode return: -119.889\t steps: 76\t Total steps: 10549\n",
      "Avg return for 25 episodes: -124.11\t Episode return: -118.857\t steps: 69\t Total steps: 10618\n",
      "Avg return for 25 episodes: -124.305\t Episode return: -111.068\t steps: 39\t Total steps: 10657\n",
      "Avg return for 25 episodes: -125.163\t Episode return: -127.202\t steps: 86\t Total steps: 10743\n",
      "Avg return for 25 episodes: -127.566\t Episode return: -165.67\t steps: 1600\t Total steps: 12343\n",
      "Avg return for 25 episodes: -128.263\t Episode return: -123.343\t steps: 47\t Total steps: 12390\n",
      "Avg return for 25 episodes: -128.682\t Episode return: -117.453\t steps: 40\t Total steps: 12430\n",
      "Avg return for 25 episodes: -129.141\t Episode return: -120.866\t steps: 43\t Total steps: 12473\n",
      "Avg return for 25 episodes: -128.541\t Episode return: -118.971\t steps: 149\t Total steps: 12622\n",
      "Avg return for 25 episodes: -123.079\t Episode return: -122.393\t steps: 44\t Total steps: 12666\n",
      "Avg return for 25 episodes: -126.113\t Episode return: -220.778\t steps: 1022\t Total steps: 13688\n",
      "Avg return for 25 episodes: -125.647\t Episode return: -122.135\t steps: 44\t Total steps: 13732\n"
     ]
    }
   ],
   "source": [
    "// Training the agent for a total of at least 20000 steps.\n",
    "train(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.Deterministic() = true;\n",
    "\n",
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment envTest(\"gym.kurg.org\", \"4040\", \"BipedalWalker-v3\");\n",
    "envTest.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {agent.Action().action};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += envTest.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "std::cout << url;\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little more training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Training the same agent for a total of at least 100000 steps.\n",
    "train(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final agent testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.Deterministic() = true;\n",
    "\n",
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment envTest(\"gym.kurg.org\", \"4040\", \"BipedalWalker-v3\");\n",
    "envTest.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {double(agent.Action().action[0] * 2)};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += envTest.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "std::cout << url;\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
