{"cells":[{"metadata":{},"cell_type":"markdown","source":"[![Binder](https://mybinder.org/badge_logo.svg)](https://lab.mlpack.org/v2/gh/mlpack/examples/master?urlpath=lab%2Ftree%2Fq_learning%2Fbipedal_walker_sac.ipynb)\n\nYou can easily run this notebook at https://lab.mlpack.org/\n\nHere, we train a [Soft Actor-Critic](https://arxiv.org/abs/1801.01290) agent to get high scores for the [Bipedal Walker](https://gym.openai.com/envs/BipedalWalker-v2/) environment. \n\nWe make the agent train and test on OpenAI Gym toolkit's GUI interface provided through a distributed infrastructure (TCP API). More details can be found [here](https://github.com/zoq/gym_tcp_api).\n\nA video of the trained agent can be seen in the end."},{"metadata":{},"cell_type":"markdown","source":"## Including necessary libraries and namespaces"},{"metadata":{"trusted":true},"cell_type":"code","source":"#include <mlpack/core.hpp>","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#include <mlpack/methods/ann/ffn.hpp>\n#include <mlpack/methods/reinforcement_learning/sac.hpp>\n#include <mlpack/methods/ann/loss_functions/empty_loss.hpp>\n#include <mlpack/methods/ann/init_rules/gaussian_init.hpp>\n#include <mlpack/methods/reinforcement_learning/environment/env_type.hpp>\n#include <mlpack/methods/reinforcement_learning/training_config.hpp>","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"// Used to run the agent on gym's environment (provided externally) for testing.\n#include <gym/environment.hpp>","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"// Used to generate and display a video of the trained agent.\n#include \"xwidgets/ximage.hpp\"\n#include \"xwidgets/xvideo.hpp\"\n#include \"xwidgets/xaudio.hpp\"","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"using namespace mlpack;","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"using namespace mlpack::ann;","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"using namespace ens;","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"using namespace mlpack::rl;","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initializing the agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"// Set up the state and action space.\nContinuousActionEnv::State::dimension = 24;\nContinuousActionEnv::Action::size = 4;","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"// Set up the actor and critic networks.\nFFN<EmptyLoss<>, GaussianInitialization>\n    policyNetwork(EmptyLoss<>(), GaussianInitialization(0, 0.1));\nFFN<EmptyLoss<>, GaussianInitialization>\n    qNetwork(EmptyLoss<>(), GaussianInitialization(0, 0.1));","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"// Set up the replay method.\nRandomReplay<ContinuousActionEnv> replayMethod(32, 10000);","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"// Set up training configurations.\nTrainingConfig config;\nconfig.ExplorationSteps() = 3200;\nconfig.TargetNetworkSyncInterval() = 1;\nconfig.UpdateInterval() = 3;","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":13,"outputs":[{"name":"stdout","output_type":"stream","text":"50policyNetwork.xml\n50qNetwork.xml\nappveyor.yml\nbipedal_walker_sac.ipynb\nbreast_cancer_wisconsin_transformation_with_pca\ncifar10_transformation_with_pca\nforest_covertype_prediction_with_random_forests\ngo\nLICENSE.txt\nlstm_electricity_consumption\nlstm_stock_prediction\nManifest.toml\nmnist_batch_norm\nmnist_cnn\nmnist_simple\nmnist_vae_cnn\nmovie_lens_prediction_with_cf\nneural_network_regression\nProject.toml\nREADME.md\nreinforcement_learning_gym\ntools\n"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data::Load(\"./50qNetwork.xml\", \"episode\", qNetwork);","execution_count":14,"outputs":[{"name":"stderr","output_type":"stream","text":"In file included from input_line_7:1:\nIn file included from /srv/conda/envs/notebook/include/mlpack/core.hpp:83:\nIn file included from /srv/conda/envs/notebook/include/mlpack/prereqs.hpp:88:\nIn file included from /srv/conda/envs/notebook/include/mlpack/core/data/has_serialize.hpp:18:\nIn file included from /srv/conda/envs/notebook/include/boost/archive/xml_oarchive.hpp:31:\nIn file included from /srv/conda/envs/notebook/include/boost/archive/basic_xml_oarchive.hpp:22:\nIn file included from /srv/conda/envs/notebook/include/boost/archive/detail/common_oarchive.hpp:22:\nIn file included from /srv/conda/envs/notebook/include/boost/archive/detail/interface_oarchive.hpp:23:\nIn file included from /srv/conda/envs/notebook/include/boost/archive/detail/oserializer.hpp:40:\nIn file included from /srv/conda/envs/notebook/include/boost/serialization/extended_type_info_typeid.hpp:32:\n/srv/conda/envs/notebook/include/boost/serialization/singleton.hpp:181:15: warning: null passed to a callee that requires a non-null argument [-Wnonnull]\n        use(* m_instance);\n              ^~~~~~~~~~\n"},{"ename":"Interpreter Exception","evalue":"","output_type":"error","traceback":["Interpreter Exception: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data::Load(\"50policyNetwork.xml\", \"episode\", policyNetwork);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing the trained agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"std::cout << \"Loading complete!\" << std::endl;\n// Set up Soft actor-critic agent.\nSAC<ContinuousActionEnv, decltype(qNetwork), decltype(policyNetwork), AdamUpdate>\n    agent(config, qNetwork, policyNetwork, replayMethod);\n\nagent.Deterministic() = true;\n\n// Creating and setting up the gym environment for testing.\ngym::Environment envTest(\"gym.kurg.org\", \"4040\", \"BipedalWalker-v3\");\nenvTest.monitor.start(\"./dummy/\", true, true);\n\n// Resets the environment.\nenvTest.reset();\nenvTest.render();\n\ndouble totalReward = 0;\nsize_t totalSteps = 0;\n\n// Testing the agent on gym's environment.\nwhile (1)\n{\n  // State from the environment is passed to the agent's internal representation.\n  agent.State().Data() = envTest.observation;\n\n  // With the given state, the agent selects an action according to its defined policy.\n  agent.SelectAction();\n\n  // Action to take, decided by the policy.\n  arma::mat action = {agent.Action().action};\n\n  envTest.step(action);\n  totalReward += envTest.reward;\n  totalSteps += 1;\n\n  if (envTest.done)\n  {\n    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n        << totalReward << std::endl;\n    break;\n  }\n\n  // Uncomment the following lines to see the reward and action in each step.\n  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n  //   << totalReward << \"\\t Action taken: \" << action;\n}\n\nenvTest.close();\nstd::string url = envTest.url();\nstd::cout << url;\nauto video = xw::video_from_url(url).finalize();\nvideo","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A little more training..."},{"metadata":{"trusted":true},"cell_type":"code","source":"// Training the same agent for a total of at least 100000 steps.\ntrain(100000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final agent testing!"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent.Deterministic() = true;\n\n// Creating and setting up the gym environment for testing.\ngym::Environment envTest(\"gym.kurg.org\", \"4040\", \"BipedalWalker-v3\");\nenvTest.monitor.start(\"./dummy/\", true, true);\n\n// Resets the environment.\nenvTest.reset();\nenvTest.render();\n\ndouble totalReward = 0;\nsize_t totalSteps = 0;\n\n// Testing the agent on gym's environment.\nwhile (1)\n{\n  // State from the environment is passed to the agent's internal representation.\n  agent.State().Data() = envTest.observation;\n\n  // With the given state, the agent selects an action according to its defined policy.\n  agent.SelectAction();\n\n  // Action to take, decided by the policy.\n  arma::mat action = {double(agent.Action().action[0] * 2)};\n\n  envTest.step(action);\n  totalReward += envTest.reward;\n  totalSteps += 1;\n\n  if (envTest.done)\n  {\n    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n        << totalReward << std::endl;\n    break;\n  }\n\n  // Uncomment the following lines to see the reward and action in each step.\n  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n  //   << totalReward << \"\\t Action taken: \" << action;\n}\n\nenvTest.close();\nstd::string url = envTest.url();\nstd::cout << url;\nauto video = xw::video_from_url(url).finalize();\nvideo","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"xcpp14","display_name":"C++14","language":"C++14"},"language_info":{"codemirror_mode":"text/x-c++src","file_extension":".cpp","mimetype":"text/x-c++src","name":"c++","version":"14"}},"nbformat":4,"nbformat_minor":2}